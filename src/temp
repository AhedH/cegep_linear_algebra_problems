  \recommended \item
    Prove each, assuming that the operations are defined, 
    where \( G \), \( H \), and 
    \( J \) are matrices, where
    \( Z \) is the zero matrix, and where \( r \) and \( s \) are scalars.
    \begin{exparts}
      \partsitem Matrix addition is commutative \( G+H=H+G \).
      \partsitem Matrix addition is associative \( G+(H+J)=(G+H)+J \).
      \partsitem The zero matrix is an additive identity \( G+Z=G \).
      \partsitem \( 0\cdot G=Z \)
      \partsitem \( (r+s)G=rG+sG \)
      \partsitem Matrices have an additive inverse \( G+(-1)\cdot G=Z \).
      \partsitem \( r(G+H)=rG+rH \)
      \partsitem \( (rs)G=r(sG) \)
    \end{exparts}
    \begin{answer}
      First, each of these properties
      is easy to check in an entry-by-entry way.
      For example, writing
      \begin{equation*}
        G=
        \begin{mat}
          g_{1,1}  &\ldots  &g_{1,n}  \\
          \vdots   &        &\vdots   \\
          g_{m,1}  &\ldots  &g_{m,n}       
        \end{mat}
        \qquad
        H=
        \begin{mat}
          h_{1,1}  &\ldots  &h_{1,n}  \\
          \vdots   &        &\vdots   \\
          h_{m,1}  &\ldots  &h_{m,n}       
        \end{mat}
      \end{equation*}
      then, by definition we have
      \begin{equation*}
        G+H=
        \begin{mat}
          g_{1,1}+h_{1,1}  &\ldots  &g_{1,n}+h_{1,n}  \\
          \vdots           &        &\vdots           \\
          g_{m,1}+h_{m,1}  &\ldots  &g_{m,n}+h_{m,n}       
        \end{mat}
      \end{equation*}
      and
      \begin{equation*}
        H+G=
        \begin{mat}
          h_{1,1}+g_{1,1}  &\ldots  &h_{1,n}+g_{1,n}  \\
          \vdots           &        &\vdots           \\
          h_{m,1}+g_{m,1}  &\ldots  &h_{m,n}+g_{m,n}       
        \end{mat}
      \end{equation*}
      and the two are equal since their entries are equal 
      $g_{i,j}+h_{i,j}=h_{i,j}+g_{i,j}$.
      That is, each of these is easy to check by using 
      \nearbydefinition{def:SumScalarProdMats} alone.

      However, each property
      is also easy to understand in terms of the represented
      maps, by applying \nearbytheorem{th:MatOpsRepMapOps} as well as
      the definition.
      \begin{exparts}
        \partsitem The two maps $g+h$ and $h+g$ are equal because
          $g(\vec{v})+h(\vec{v})=h(\vec{v})+g(\vec{v})$, as addition is 
          commutative in any vector space.
          Because the maps are the same, they must have the same
          representative. 
        \partsitem As with the prior answer, except that here we apply that
          vector space addition is associative.
        \partsitem As before, except that here we note that
          $g(\vec{v})+z(\vec{v})=g(\vec{v})+\zero=g(\vec{v})$.
        \partsitem Apply that
          $0\cdot g(\vec{v})=\zero=z(\vec{v})$.
        \partsitem Apply that
          $(r+s)\cdot g(\vec{v})=r\cdot g(\vec{v})+s\cdot g(\vec{v})$.
        \partsitem Apply the prior two items with $r=1$ and $s=-1$.
        \partsitem Apply that
          $r\cdot (g(\vec{v})+h(\vec{v}))=r\cdot g(\vec{v})+r\cdot h(\vec{v})$.
        \partsitem Apply that
          $(rs)\cdot g(\vec{v})=r\cdot(s\cdot g(\vec{v}))$.
      \end{exparts}
    \end{answer}
   \item 
    The \definend{trace}\index{trace}\index{matrix!trace} 
    of a square matrix is the sum of the entries on the main diagonal (the
    \( 1,1 \) entry
    plus the \( 2,2 \) entry, etc.;
    we will see the significance of the trace in Chapter Five).
    Show that \( \mbox{trace}(H+G)=\mbox{trace}(H)+\mbox{trace}(G)  \).
    Is there a similar result for scalar multiplication?
    \begin{answer}
      That the trace of a sum is the sum of the traces holds
      because both \( \text{trace}(H+G) \) and
      \( \text{trace}(H)+\text{trace}(G) \) are the sum of
      \( h_{1,1}+g_{1,1} \) with \( h_{2,2}+g_{2,2} \), etc.
      For scalar multiplication we have
      \( \mbox{trace}(r\cdot H)=r\cdot\mbox{trace}(H) \); the proof is easy.
      Thus the trace map is a homomorphism from $\matspace_{\nbyn{n}}$ to
      $\Re$.  
    \end{answer}
  \item 
    Recall that the \definend{transpose}\index{matrix!transpose}%
    \index{transpose!interaction with sum and scalar multiplication}
    of a matrix $M$ is another matrix, whose $i,j$ entry is the 
    $j,i$ entry of $M$.
    Verify these identities.
    \begin{exparts}
      \partsitem \( \trans{(G+H)}=\trans{G}+\trans{H} \)
      \partsitem \( \trans{(r\cdot H)}=r\cdot\trans{H} \)
    \end{exparts}
    \begin{answer} 
      \begin{exparts}
        \partsitem The \( i,j \) entry of \( \trans{(G+H)} \) is
          \( g_{j,i}+h_{j,i} \).
          That is also the \( i,j \) entry of \( \trans{G}+\trans{H} \).
        \partsitem The \( i,j \) entry of \( \trans{(r\cdot H)} \) is
          \( rh_{j,i} \),
          which is also the \( i,j \) entry of \( r\cdot\trans{H} \).
      \end{exparts}  
   \end{answer}
  \recommended \item 
    A square matrix is \definend{symmetric}\index{matrix!symmetric}%
    \index{symmetric matrix} if each \( i,j \) entry equals
    the \( j,i \) entry, that is, if the matrix equals its transpose.
    \begin{exparts}
      \partsitem Prove that for any square~$H$,
        the matrix \( H+\trans{H} \) is symmetric.
        Does every symmetric matrix have this form?
      \partsitem Prove that the set of \( \nbyn{n} \) symmetric matrices is
        a subspace of \( \matspace_{\nbyn{n}} \).
    \end{exparts}
    \begin{answer}  
      \begin{exparts}
        \partsitem For \( H+\trans{H} \), the \( i,j \) entry 
          is \( h_{i,j}+h_{j,i} \) and
          the \( j,i \) entry of  is \( h_{j,i}+h_{i,j} \).
          The two are equal and thus \( H+\trans{H} \) is symmetric.

          Every symmetric matrix does have that form, since we can write
          \( H=(1/2)\cdot(H+\trans{H}) \).
        \partsitem The set of symmetric matrices is nonempty as it
          contains the zero matrix.
          Clearly a scalar multiple of a symmetric matrix is symmetric.
          A sum \( H+G \) of two symmetric matrices is
          symmetric because \( h_{i,j}+g_{i,j}=h_{j,i}+g_{j,i} \) (since
          \( h_{i,j}=h_{j,i} \) and \( g_{i,j}=g_{j,i} \)).
          Thus the subset is nonempty and closed under the inherited
          operations, and so it is a subspace.
      \end{exparts} 
    \end{answer}



  \item  
    Which products are defined?
    \begin{exparts*}
      \partsitem \( \nbym{3}{2} \)~times~\( \nbym{2}{3} \)
      \partsitem \( \nbym{2}{3} \)~times~\( \nbym{3}{2} \)
      \partsitem \( \nbym{2}{2} \)~times~\( \nbym{3}{3} \)
      \partsitem \( \nbym{3}{3} \)~times~\( \nbym{2}{2} \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts*}
        \partsitem Yes.
        \partsitem Yes.
        \partsitem No.
        \partsitem No.
      \end{exparts*}
    \end{answer}
  \recommended \item 
    Give the size of the product or state ``not defined''.
    \begin{exparts}
      \partsitem a \( \nbym{2}{3} \) matrix times a \( \nbym{3}{1} \) matrix
      \partsitem a \( \nbym{1}{12} \) matrix times a \( \nbym{12}{1} \) matrix
      \partsitem a \( \nbym{2}{3} \) matrix times a \( \nbym{2}{1} \) matrix
      \partsitem a \( \nbym{2}{2} \) matrix times a \( \nbym{2}{2} \) matrix
    \end{exparts}
    \begin{answer}  
      \begin{exparts*}
        \partsitem \( \nbym{2}{1} \)
        \partsitem \( \nbym{1}{1} \)
        \partsitem Not defined.
        \partsitem \( \nbym{2}{2} \)
      \end{exparts*}  
     \end{answer}
  \recommended \item \label{exer:NicePropsMatMult} 
    \begin{exparts}
      \partsitem Prove that $H^pH^q=H^{p+q}$ and $(H^p)^q=H^{pq}$ 
        for positive integers \( p,q \).
      \partsitem Prove that $(rH)^p=r^p\cdot H^p$ 
        for any positive integer \( p \) and scalar \( r\in\Re \).
    \end{exparts}
    \begin{answer}
      Each follows easily from the associated map fact.
      For instance, $p$ applications of the transformation $h$, following $q$
      applications, is simply $p+q$ applications.
    \end{answer}
  \recommended \item \label{exer:MoreNicePropsMatMult} 
    \begin{exparts}
      \partsitem How does matrix multiplication interact with 
        scalar multiplication:~is \( r(GH)=(rG)H \)?
        Is \( G(rH)=r(GH) \)?
      \partsitem  How does matrix multiplication interact with 
        linear combinations:~is \( F(rG+sH)=r(FG)+s(FH) \)?
        Is $(rF+sG)H=rFH+sGH$?
    \end{exparts}
    \begin{answer}  
      Although we can do these by going through the indices, they
      are best understood in terms of the represented maps.
      That is, fix spaces and bases so that the matrices 
      represent linear maps $f,g,h$.
       \begin{exparts}
        \partsitem Yes;  we have both 
          $r\cdot (\composed{g}{h})\,(\vec{v})
           =r\cdot g(\,h(\vec{v})\,)
           =\composed{(r\cdot g)}{h}\,(\vec{v})$
          and 
          $\composed{g}{(r\cdot h)}\,(\vec{v})=g(\,r\cdot h(\vec{v})\,)
            =r\cdot g(h(\vec{v}))
            =r\cdot (\composed{g}{h})\,(\vec{v})$
          (the second equality holds because of the linearity of $g$).
        \partsitem Both answers are yes.
          First, $\composed{f}{(rg+sh)}$ and
          $r\cdot(\composed{f}{g})+s\cdot(\composed{f}{h})$ both send 
          $\vec{v}$ to $r\cdot f(g(\vec{v}))+s\cdot f(h(\vec{v}))$; the
          calculation is as in the prior item
          (using the linearity of $f$ for the first one). 
          For the other, 
          $\composed{(rf+sg)}{h}$ and 
          $r\cdot(\composed{f}{h})+s\cdot(\composed{g}{h})$ both send
          $\vec{v}$ to $r\cdot f(h(\vec{v}))+s\cdot g(h(\vec{v}))$.
      \end{exparts}  
    \end{answer}
  \item \label{exer:TranspAndMult} 
    We can ask how the matrix product 
    operation interacts with the transpose operation. 
    \begin{exparts}  
      \partsitem Show that \( \trans{(GH)}=\trans{H}\trans{G} \).
      \partsitem A square matrix is 
        \definend{symmetric}\index{symmetric matrix}%
        \index{matrix!symmetric} if each 
        \( i,j \) entry equals the
        \( j,i \) entry, that is, if the matrix equals its own transpose.
        Show that
        the matrices \( H\trans{H} \) and \( \trans{H}H \) are symmetric.
      %\partsitem Is every symmetric matrix of that form?
    \end{exparts}
    \begin{answer}
      We have not seen a map interpretation of the transpose operation, so
      we will verify these by considering the entries.
      \begin{exparts}
        \partsitem  The \( i,j \) entry of \( \trans{GH} \) is the $j,i$ entry
          of $GH$, which is the dot product of the
          \( j \)-th row of \( G \) and the \( i \)-th column of \( H \).
          The \( i,j \) entry of \( \trans{H}\trans{G} \) is the dot product of
          the \( i \)-th row of \( \trans{H} \) and the \( j \)-th column of
          \( \trans{G} \), which is the
          dot product of the \( i \)-th column of \( H \) and the
          \( j \)-th row of \( G \).
          Dot product is commutative and so these two are equal.
        \partsitem By the prior item each equals its transpose, e.g.,
          $\trans{(H\trans{H})}=\trans{\trans{H}}\trans{H}=H\trans{H}$.
      \end{exparts}  
    \end{answer}



\begin{exercises}
  \recommended \item
    Predict the result of each multiplication by an elementary
    reduction matrix, and then check by multiplying it out.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
                 3  &0  \\
                 0  &1
               \end{mat}
               \begin{mat}[r]
                 1  &2  \\
                 3  &4
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                 1  &0  \\
                 0  &2
               \end{mat}
               \begin{mat}[r]
                 1  &2  \\
                 3  &4
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                 1  &0  \\
                -2  &1
               \end{mat}
               \begin{mat}[r]
                 1  &2  \\
                 3  &4
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                 1  &2  \\
                 3  &4
               \end{mat}
               \begin{mat}[r]
                 1  &-1 \\
                 0  &1
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                 1  &2  \\
                 3  &4
               \end{mat}
               \begin{mat}[r]
                 0  &1  \\
                 1  &0
               \end{mat} \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem The second matrix has its first row multiplied 
          by \( 3 \).
          \begin{equation*}
            \begin{mat}[r]
              3  &6  \\
              3  &4
            \end{mat}
          \end{equation*}
        \partsitem The second matrix has its second row multiplied by \( 2 \).
          \begin{equation*}
            \begin{mat}[r]
              1  &2  \\
              6  &8
            \end{mat}
          \end{equation*}
        \partsitem The second matrix undergoes the combination operation
          of replacing the second row with \( -2 \) times the first row added
          to the second.
          \begin{equation*}
            \begin{mat}[r]
              1  &2  \\
              1  &0
            \end{mat}
          \end{equation*}
        \partsitem The first matrix undergoes the column operation of:~replace
          the
          second column by $-1$ times the first column plus the
          second.
          \begin{equation*}
            \begin{mat}[r]
              1  &1  \\
              3  &1
            \end{mat}
          \end{equation*}
        \partsitem The first matrix has its columns swapped.
          \begin{equation*}
            \begin{mat}[r]
              2  &1  \\
              4  &3
            \end{mat}
          \end{equation*}
      \end{exparts}   
    \end{answer}
  \item
    Predict the result of each multiplication by a
    diagonal matrix, and then check by multiplying it out.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
                 3  &0  \\
                 0  &0
               \end{mat}
               \begin{mat}[r]
                 1  &2  \\
                 3  &4
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                 4  &0  \\
                 0  &2
               \end{mat}
               \begin{mat}[r]
                 1  &2  \\
                 3  &4
               \end{mat} \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem The second matrix has its first row multiplied 
          by \( 3 \) and
          its second row multiplied by \( 0 \).
          \begin{equation*}
            \begin{mat}[r]
              3  &6  \\
              0  &0
            \end{mat}
          \end{equation*}
        \partsitem The second matrix has its first row multiplied 
          by \( 4 \) and
          its second row multiplied by \( 2 \).
          \begin{equation*}
            \begin{mat}[r]
              4  &8  \\
              6  &8
            \end{mat}
          \end{equation*}
      \end{exparts}   
    \end{answer}
  \item Produce each.
    \begin{exparts}
      \partsitem a $\nbyn{3}$ matrix that, acting from the left,
         swaps rows one and two
      \partsitem a $\nbyn{2}$ matrix that, acting from the right,
         swaps column one and two
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem This matrix swaps row one and row three.
          \begin{equation*}
            \begin{mat}
              0  &1 &0 \\
              1  &0 &0 \\
              0  &0 &1
            \end{mat}
            \begin{mat}
              a &b &c \\
              d &e &f \\
              g &h &i
            \end{mat}
            =
            \begin{mat}
              d &e &f \\
              a &b &c \\
              g &h &i
            \end{mat}         
          \end{equation*}
        \partsitem This matrix swaps column one and two.
          \begin{equation*}
            \begin{mat}
              a  &b  \\
              c  &d
            \end{mat}
            \begin{mat}
              0  &1  \\
              1  &0   
            \end{mat}
            =
            \begin{mat}
              b  &a  \\
              d  &c
            \end{mat}
          \end{equation*}
      \end{exparts}
    \end{answer}
  \recommended \item
    This table gives the number of hours of each 
    type done by each worker, and the associated pay rates.
    Use matrices to compute the wages due.
    \begin{center}
      \begin{tabular}[t]{l|cc}
        \multicolumn{1}{c}{\ }  
         &\multicolumn{1}{c}{\textit{regular}} 
         &\multicolumn{1}{c}{\textit{overtime}}  \\
        \cline{2-3}
        Alan      &40        &12        \\
        Betty     &35        &6         \\  
        Catherine &40        &18         \\  
        Donald    &28        &0         %\\  \cline{2-3}
      \end{tabular}
      \qquad
      \begin{tabular}[t]{l|c}
        \multicolumn{1}{c}{\ }   &\multicolumn{1}{c}{\textit{wage}}   \\
        \cline{2-2}
        regular   &$\$ 25.00$  \\
        overtime  &$\$ 45.00$  %\\  \cline{2-2}
      \end{tabular}
    \end{center}
    \textit{Remark.}
    This illustrates that in practice
    we often want to compute linear combinations of rows and
    columns in a context where we really aren't interested in any
    associated linear maps.
    \begin{answer}
      The pay due each person appears in the matrix product of the two
      arrays. 
    \end{answer}
  \item 
    Find the product of this matrix with its transpose.
    \begin{equation*}
      \begin{mat}
        \cos\theta  &-\sin\theta  \\
        \sin\theta  &\cos\theta
      \end{mat}
    \end{equation*}
    \begin{answer}
      The product is the identity matrix (recall that
      $\cos^2\theta+\sin^2\theta =1$).
      An explanation is that the given matrix represents, with respect to
      the standard bases, a rotation in \( \Re^2 \) of \( \theta \)
      radians while the transpose represents a rotation of \( -\theta \)
      radians.
      The two cancel.  
     \end{answer}
  \recommended \item 
    The need to take linear combinations of rows and columns in tables of 
    numbers arises often in practice.
    For instance, this is a map of part of Vermont and New York.
    %\typeout{Ignore the overfull box here; it is bogus.}
    \begin{center}
      \parbox{2in}{In part because of Lake Champlain, there are
                      no roads directly connecting some pairs of towns.
                      For instance, there is no way to go from
                      Winooski to Grand Isle without going through
                      Colchester.
                      (To simplify the graph many other roads and towns
                      have been omitted.
                      From top to bottom of this map is
                      about forty miles.)}
      \quad
      \parbox{2in}{\includegraphics{ch3.99}}
    \end{center}
    \begin{exparts}
      \partsitem The \definend{adjacency matrix}\index{adjacency matrix}%
        \index{matrix!adjacency} 
        of a map is the square matrix
        whose \( i,j \) entry is the number of roads from city \( i \)
        to city \( j \).
        Produce the incidence matrix of this map (take the cities in
        alphabetical order).
      \partsitem A matrix is 
        \definend{symmetric}\index{matrix!symmetric} 
        if it equals its transpose.
        Show that an adjacency matrix is symmetric.
        (These are all two-way streets.
        Vermont doesn't have many one-way streets.)
      \partsitem What is the significance of the square of the 
        incidence matrix?
        The cube?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem The adjacency matrix is this
          (e.g, the first row shows that there is only one connection
          including Burlington, the road to Winooski).
          \begin{equation*}
            \begin{mat}[r]
              0  &0  &0  &0  &1  \\
              0  &0  &1  &1  &1  \\
              0  &1  &0  &1  &0  \\
              0  &1  &1  &0  &0  \\
              1  &1  &0  &0  &0
            \end{mat}
          \end{equation*}
        \partsitem Because these are two-way roads, 
          any road connecting city~\( i \)
          to city~\( j \) gives a connection between city~\( j \) and 
          city~\( i \).
        \partsitem The square of the adjacency matrix tells how cities
          are connected
          by trips involving two roads.
      \end{exparts}  
   \end{answer}
  \recommended \item 
    Prove that the diagonal matrices form a subspace of
    \( \matspace_{\nbyn{n}} \).
    What is its dimension?
    \begin{answer}
      The set of diagonal matrices is nonempty as the zero matrix is
      diagonal.
      Clearly it is closed under scalar multiples and sums.
      Therefore it is a subspace.
      The dimension is \( n \); here is a basis.
      \begin{equation*}
        \set{\begin{mat}
               1  &0  &\ldots    \\
               0  &0             \\
                  &   &\ddots    \\
               0  &0  &      &0
             \end{mat},\ldots,
             \begin{mat}
               0  &0  &\ldots    \\
               0  &0             \\
                  &   &\ddots    \\
               0  &0  &      &1
             \end{mat}  }
      \end{equation*} 
    \end{answer}
  \item 
    Does the identity matrix represent the identity map if the bases are
    unequal?
    \begin{answer}
      No.
      In \( \polyspace_1 \), with respect to the unequal bases
      \( B=\sequence{1,x} \) and \( D=\sequence{1+x,1-x} \),
      the identity transformation is represented by this matrix.
      \begin{equation*}
         \rep{\text{id}}{B,D}=
         \begin{mat}[r]
           1/2  &1/2  \\
           1/2  &-1/2
         \end{mat}_{B,D}
      \end{equation*}   
    \end{answer}
  \item 
    Show that every multiple of the identity commutes with every
    square matrix.
    Are there other matrices that commute with all square matrices?
    \begin{answer}
      For any scalar \( r \) and square matrix \( H \) we have
      \( (rI)H=r(IH)=rH=r(HI)=(Hr)I=H(rI) \).

      There are no other such matrices; here is an argument for $\nbyn{2}$ 
      matrices that is easily extended to $\nbyn{n}$.
      If a matrix commutes with all others then it commutes with this
      unit matrix.
      \begin{equation*}
        \begin{mat}
          0  &a  \\
          0  &c 
        \end{mat}
        =\begin{mat}
          a  &b  \\
          c  &d   
        \end{mat}
        \begin{mat}[r]
          0  &1  \\
          0  &0
        \end{mat}
        =\begin{mat}[r]
          0  &1  \\
          0  &0
        \end{mat}
        \begin{mat}
          a  &b  \\
          c  &d   
        \end{mat}
        =\begin{mat}
          c  &d  \\
          0  &0 
        \end{mat}
      \end{equation*}
      From this we first conclude that the upper left entry~$a$ 
      must equal its lower right entry~$d$.
      We also conclude that the lower left entry~$c$ is zero.
      The argument for the upper right entry~$b$ is similar.
    \end{answer}
  \item 
    Prove or disprove: nonsingular matrices commute.
    \begin{answer}
      It is false; these two don't commute.
      \begin{equation*}
         \begin{mat}[r]
           1  &2  \\
           3  &4
         \end{mat}
         \qquad
         \begin{mat}[r]
           5  &6  \\
           7  &8
         \end{mat}
      \end{equation*}  
    \end{answer}
  \recommended \item \label{exer:PermTimesTransEqId}
    Show that the product of a permutation matrix and its transpose
    is an identity matrix.
    \begin{answer}
      A permutation matrix has a single one in each row and column, and
      all its other entries are zeroes.
      Fix such a matrix.
      Suppose that the \( i \)-th row has its one in its \( j \)-th column.
      Then no other row has its one in the \( j \)-th column; every other
      row has a zero in the \( j \)-th column.
      Thus the dot product of the \( i \)-th row and any other row is zero.

      The \( i \)-th row of the product is made up of the dot products of the
      \( i \)-th row of the matrix and the columns of the transpose.
      By the last paragraph, all such dot products are zero except for the
      \( i \)-th one, which is one.  
    \end{answer}
  \item 
    Show that if the first and second rows of \( G \) are equal then so
    are the first and second rows of \( GH \).
    Generalize.
    \begin{answer}
      The generalization is to go from the first and second rows to the
      $i_1$-th and $i_2$-th rows.
      Row~$i$ of \( GH \) is made up of the dot products of
      row~$i$ of \( G \) and the columns of \( H \).
      Thus if rows \( i_1 \) and \( i_2 \) of \( G \) are equal then so are
      rows \( i_1 \) and \( i_2 \) of \( GH \).  
    \end{answer}
  \item 
    Describe the product of two diagonal matrices.
    \begin{answer}
      If the product of two diagonal matrices is defined\Dash if 
      both are $\nbyn{n}$\Dash then 
      the product of the diagonals is the diagonal
      of the products:~where \( G,H \) are equal-sized diagonal matrices,
      \( GH \) is all zeros except each that \( i,i \) entry is
      \( g_{i,i}h_{i,i} \).
    \end{answer}
  \item 
    Write
    \begin{equation*}
      \begin{mat}[r]
        1   &0  \\
        -3  &3
      \end{mat}
    \end{equation*}
    as the product of two elementary reduction matrices.
    \begin{answer}
      One way to produce this matrix from the identity is to use 
      the column operations
      of first multiplying the second column by three, and then adding the 
      negative of the resulting second column to the first.
      \begin{equation*}
        \begin{mat}[r]
          1  &0  \\
          0  &1
        \end{mat}
        \grstep{}
        \begin{mat}[r]
          1  &0  \\
          0  &3
        \end{mat}
        \grstep{}
        \begin{mat}[r]
          1  &0  \\
          -3  &3
        \end{mat}
      \end{equation*}
      In contrast with row operations, column operations are written from
      left to right, so this matrix product expresses
      doing the above two operations.
      \begin{equation*}
        \begin{mat}[r]
          1  &0  \\
          0  &3
        \end{mat}
        \begin{mat}[r]
          1  &0  \\
         -1  &1
        \end{mat}
      \end{equation*}
      \textit{Remark.}
      Alternatively, we could get the required matrix with row operations.
      Starting with the identity, first adding the negative of the first 
      row to the  second, and then multiplying the second row by three
      will work.
      Because we write successive row operations as matrix products from
      right to left, doing these two row operations is expressed with:~the 
      same matrix product.    
    \end{answer}
  \recommended \item 
    Show that if \( G \) has a row of zeros then \( GH \)
    (if defined) has a row of zeros.
    Does that work for columns?
    \begin{answer}
      The \( i \)-th row of \( GH \) is made up of the dot products of
      the \( i \)-th row of \( G \) with the columns of \( H \).
      The dot product of a zero row with a column is zero.

      It works for columns if stated correctly:~if \( H \) has a column of
      zeros then \( GH \) (if defined) has a column of zeros.
      The proof is easy.  
    \end{answer}
  \item 
    Show that the set of unit matrices forms a basis for 
    \( \matspace_{\nbym{n}{m}} \).
    \begin{answer}
      Perhaps the easiest way is to show that each \( \nbym{n}{m} \) matrix
      is a linear combination of unit matrices in one and only one way:
      \begin{equation*}
        c_1\begin{mat}
             1  &0  &\ldots  \\
             0  &0           \\
             \vdots
           \end{mat}
        +\dots+
        c_{n,m}\begin{mat}
             0  &0      &\ldots  \\
             \vdots              \\
             0  &\ldots &       &1
           \end{mat}
        =\begin{mat}
           a_{1,1} &a_{1,2} &\ldots          \\
           \vdots                            \\
           a_{n,1} &\ldots  &      &a_{n,m} 
         \end{mat}
      \end{equation*}
      has the unique solution \( c_1=a_{1,1} \), \( c_2=a_{1,2} \),
      etc.  
    \end{answer}
  \item  
    Find the formula for the $n$-th power of this matrix.
    \begin{equation*}
      \begin{mat}[r]
        1  &1  \\
        1  &0
      \end{mat}
    \end{equation*}
    \begin{answer}
      Call that matrix \( F \).
      We have
      \begin{equation*}
        F^2=\begin{mat}[r]
          2  &1  \\
          1  &1
        \end{mat}
        \quad
        F^3=\begin{mat}[r]
          3  &2  \\
          2  &1
        \end{mat}
        \quad
        F^4=\begin{mat}[r]
          5  &3  \\
          3  &2
        \end{mat}
      \end{equation*}
      In general, 
      \begin{equation*}
        F^n=\begin{mat}
          f_{n+1} &f_n  \\
          f_n     &f_{n-1}
        \end{mat}
      \end{equation*}
      where \( f_i \) is the \( i \)-th Fibonacci number
      \( f_i=f_{i-1}+f_{i-2} \) and \( f_0=0 \), \( f_1=1 \), 
      which we verify by induction, based on this equation.
      \begin{equation*}
        \begin{mat}
          f_{i-1}  &f_{i-2} \\
          f_{i-2}  &f_{i-3}
        \end{mat}
        \begin{mat}[r]
          1  &1  \\
          1  &0
        \end{mat}
        =\begin{mat}
           f_i     &f_{i-1}  \\
           f_{i-1} &f_{i-2}
        \end{mat}
      \end{equation*}
    \end{answer}
  \recommended \item
   The \definend{trace}\index{trace}\index{matrix!trace} 
   of a square matrix is the sum of the
   entries on its diagonal (its significance appears in
   Chapter Five).
   Show that \( \trace (GH)=\trace (HG)  \).
   \begin{answer}
     \textit{Chapter Five gives a less computational reason\Dash the
     trace of a matrix is the second coefficient in its characteristic
     polynomial\Dash but for now we can use indices.}
     We have
     \begin{align*}
       \trace (GH)
       &=(g_{1,1}h_{1,1}+g_{1,2}h_{2,1}+\dots+g_{1,n}h_{n,1})  \\
       &\text{}\quad+(g_{2,1}h_{1,2}+g_{2,2}h_{2,2}+\dots+g_{2,n}h_{n,2}) \\
       &\text{}\quad
        +\cdots+(g_{n,1}h_{1,n}+g_{n,2}h_{2,n}+\dots+g_{n,n}h_{n,n})
     \end{align*}
     while
     \begin{align*}
       \trace (HG)
       &=(h_{1,1}g_{1,1}+h_{1,2}g_{2,1}+\dots+h_{1,n}g_{n,1})  \\
       &\text{}\quad+(h_{2,1}g_{1,2}+h_{2,2}g_{2,2}+\dots+h_{2,n}g_{n,2}) \\
       &\text{}\quad
         +\cdots+(h_{n,1}g_{1,n}+h_{n,2}g_{2,n}+\dots+h_{n,n}g_{n,n})
     \end{align*}
     and the two are equal.  
    \end{answer}
  \recommended \item
    A square matrix is 
    \definend{upper triangular}\index{triangular matrix}%
    \index{matrix!triangular} 
    if its only nonzero
    entries lie above, or on, the diagonal.
    Show that the product of two upper triangular matrices is upper triangular.
    Does this hold for lower triangular also?
    \begin{answer}
      A matrix is upper triangular if and only if its $i,j$ entry is zero 
      whenever \( i>j \).
      Thus, if \( G,H \) are upper triangular then \( h_{i,j} \) and
      \( g_{i,j} \) are zero when \( i>j \).
      An entry in the product
      \( p_{i,j}=g_{i,1}h_{1,j}+\dots+g_{i,n}h_{n,j} \)
      is zero unless at least some of the terms are nonzero, that is, unless
      for at least some of the summands
      \( g_{i,r}h_{r,j} \) both \( i\leq r \) and \( r\leq j \).
      Of course, if \( i>j \) this cannot happen and so the product of two
      upper triangular matrices is upper triangular.
      (A similar argument works for lower triangular matrices.) 
   \end{answer}
 \item 
   A square matrix is a \definend{Markov matrix}\index{matrix!Markov} 
   if each entry is between zero
   and one and the sum along each row is one.
   Prove that a product of Markov matrices is Markov.
   \begin{answer}
     The sum along the \( i \)-th row of the product is this.
     \begin{align*}
       p_{i,1}+\cdots+p_{i,n}
       &=(h_{i,1}g_{1,1}+h_{i,2}g_{2,1}+\dots+h_{i,n}g_{n,1})  \\
       &\text{}\quad+(h_{i,1}g_{1,2}+h_{i,2}g_{2,2}+\dots+h_{i,n}g_{n,2}) \\
       &\text{}\quad
        +\dots+(h_{i,1}g_{1,n}+h_{i,2}g_{2,n}+\dots+h_{i,n}g_{n,n})  \\
       &=h_{i,1}(g_{1,1}+g_{1,2}+\dots+g_{1,n})  \\
       &\text{}\quad+h_{i,2}(g_{2,1}+g_{2,2}+\dots+g_{2,n}) \\
       &\text{}\quad
        +\dots+h_{i,n}(g_{n,1}+g_{n,2}+\dots+g_{n,n})  \\
       &=h_{i,1}\cdot 1+\dots+h_{i,n}\cdot 1  \\
       &=1
     \end{align*}  
    \end{answer}
  \recommended \item
    Give an example of two matrices of the same rank and size
    with squares of differing rank.
    \begin{answer}
      Matrices representing (say, with respect to
      \( \stdbasis_2,\stdbasis_2\subset\Re^2 \)) the maps that send
      \begin{equation*}
        \vec{\beta}_1 \mapsunder{h} \vec{\beta}_1
        \quad
        \vec{\beta}_2 \mapsunder{h} \zero
      \end{equation*}
      and
      \begin{equation*}
        \vec{\beta}_1 \mapsunder{g} \vec{\beta}_2
        \quad
        \vec{\beta}_2 \mapsunder{g} \zero
      \end{equation*}
      will do.  
    \end{answer}
  \item 
    Combine the two generalizations of the identity matrix,
    the one allowing entries to be other than ones, and the one allowing the
    single one in each row and column to be off the diagonal.
    What is the action of this type of matrix?
    \begin{answer}
      The combination is to have all entries of the matrix be zero
      except for one (possibly) nonzero entry in each row and column.
      We can write such a matrix as the product of a permutation matrix and
      a diagonal matrix, e.g.,
      \begin{equation*}
        \begin{mat}[r]
          0  &4  &0  \\
          2  &0  &0  \\
          0  &0  &-5
        \end{mat}
        =\begin{mat}[r]
          0  &1  &0  \\
          1  &0  &0  \\
          0  &0  &1
        \end{mat}
        \begin{mat}[r]
          4  &0  &0  \\
          0  &2  &0  \\
          0  &0  &-5
        \end{mat}
      \end{equation*}
      and its action is thus to rescale the rows and permute them.
    \end{answer}
  \item 
    On a computer multiplications have traditionally been 
    more costly than additions, so
    people have tried to in reduce the number of multiplications used to
    compute a matrix product.
    \begin{exparts}
      \partsitem How many real number multiplications do we need in the formula 
        we gave for the product of a
        \( \nbym{m}{r} \) matrix and a \( \nbym{r}{n} \) matrix?
      \partsitem 
        Matrix multiplication is associative, so all associations yield
        the same result.
        The cost in number of multiplications, however, varies.
        Find the association requiring the fewest real number multiplications
        to compute the matrix product of
        a \( \nbym{5}{10} \) matrix,
        a \( \nbym{10}{20} \) matrix,
        a \( \nbym{20}{5} \) matrix, and
        a \( \nbym{5}{1} \) matrix.
      \partsitem \textit{(Very hard.)}
        Find a way to multiply two \( \nbyn{2} \) matrices using only seven
        multiplications instead of the eight suggested by the naive approach.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Each entry \(p_{i,j}=g_{i,1}h_{1,j}+\dots+g_{1,r}h_{r,1}  \)
          takes \( r \) multiplications and there are \( m\cdot n \) entries.
          Thus there are \( m\cdot n\cdot r \) multiplications.
        \partsitem Let \( H_1 \) be \( \nbym{5}{10} \),
           let \( H_2 \) be \( \nbym{10}{20} \),
           let \( H_3 \) be \( \nbym{20}{5} \),
           let \( H_4 \) be \( \nbym{5}{1} \).
           Then, using the formula from the prior part, 
           \begin{center}
             \begin{tabular}{l|l}
               \multicolumn{1}{c}{\textit{this association}}  
                &\multicolumn{1}{c}{\textit{uses this many multiplications}}\\ 
               \hline
               \( ((H_1H_2)H_3)H_4 \)     &\( 1000+500+25=1525 \)  \\
               \( (H_1(H_2H_3))H_4 \)     &\( 1000+250+25=1275 \)  \\
               \( (H_1H_2)(H_3H_4) \)     &\( 1000+100+100=1200 \)  \\
               \( H_1(H_2(H_3H_4)) \)     &\( 100+200+50=350    \)  \\
               \( H_1((H_2H_3)H_4) \)     &\( 1000+50+50=1100    \)
             \end{tabular}
           \end{center}
           shows which is cheapest.
         \partsitem This is an improvement by S.~Winograd of a formula due to
           V.~Strassen:
           let \( w=aA-(a-c-d)(A-C+D) \) and then
           \begin{equation*}
               \begin{mat}
                 a  &b  \\
                 c  &d
               \end{mat}
               \begin{mat}
                 A  &B  \\
                 C  &D
                 \end{mat}    \\
               =
               \begin{mat}
                 \alpha  &\beta  \\
                 \gamma  &\delta
               \end{mat}
               \end{equation*}
             where $\alpha=aA+bB$,
             and $\beta=w+(c+d)(C-A)+(a+b-c-d)D$,
             and $\gamma=w+(a-c)(D-C)-d(A-B-C+D)$,
             and $\delta=w+(a-c)(D-C)+(c+d)(C-A)$.
               % \begin{mat}
               %    aA+bB
               %    &w+(c+d)(C-A)+(a+b-c-d)D \\
               %    w+(a-c)(D-C)-d(A-B-C+D)
               %    &w+(a-c)(D-C)+(c+d)(C-A)
               % \end{mat}
           This takes seven multiplications and fifteen additions (save the
           intermediate results).
      \end{exparts}  
    \end{answer}
  \puzzle \item  
    \cite{Putnam90A5}
    If \( A \) and \( B \) are square matrices of the
    same size such that \( ABAB=0 \), does it follow that \( BABA=0 \)?
    \begin{answer}
      \answerasgiven 
      No, it does not.
      Let \( A \) and \( B \) represent, with respect to the standard bases,
      these transformations of \( \Re^3 \).
      \begin{equation*}
        \colvec{x \\ y \\ z}\mapsunder{a}\colvec{x \\ y \\ 0}
        \qquad
        \colvec{x \\ y \\ z}\mapsunder{a}\colvec{0 \\ x \\ y}
      \end{equation*}
      Observe that
      \begin{equation*}
        \colvec{x \\ y \\ z}\mapsunder{abab}\colvec[r]{0 \\ 0 \\ 0}
        \quad\text{but}\quad
        \colvec{x \\ y \\ z}\mapsunder{baba}\colvec{0 \\ 0 \\ x}.
      \end{equation*} 
    \end{answer}
  \item  
    \cite{Monthly66p1114}
    Demonstrate these four assertions to get an
    alternate proof that column rank equals row rank.
    \begin{exparts}
      \partsitem \( \vec{y}\cdot\vec{y}=0 \) iff \( \vec{y}=\zero \).
      \partsitem \( A\vec{x}=\zero \) iff \( \trans{A}A\vec{x}=\zero \).
      \partsitem \( \dim(\rangespace{A})=\dim(\rangespace{\trans{A}A}) \).
      \partsitem \( \text{col rank}(A)=\text{col rank}(\trans{A})
        =\text{row rank}(A) \).
    \end{exparts}
    \begin{answer}
      \answerasgiven
      \begin{exparts}
        \partsitem Obvious.
        \partsitem If \( \trans{A}A\vec{x}=\zero \) then 
          \( \vec{y}\cdot\vec{y}=0 \)
          where \( \vec{y}=A\vec{x} \).
          Hence \( \vec{y}=\zero \) by (a).

          The converse is obvious.
        \partsitem By (b), \( A\vec{x}_1 \),\ldots,\( A\vec{x}_n \) 
          are linearly
          independent iff \( \trans{A}A\vec{x}_1 \),\ldots,
          \( \trans{A}A\vec{v}_n \) are linearly independent.
        \partsitem We have 
          \begin{multline*}
            \text{col rank}(A)=\text{col rank}(\trans{A}A)
            =\dim\set{\trans{A}(A\vec{x})\suchthat \text{all \(\vec{x}\)} }  \\
            \leq \dim\set{\trans{A}\vec{y}\suchthat \text{all \(\vec{y}\)} }
            =\text{col rank}(\trans{A}).
          \end{multline*}
          Thus also \( \text{col rank}(\trans{A})
                       \leq\text{col rank}(\trans{\trans{A}}) \)
          and so \( \text{col rank}(A)=\text{col rank}(\trans{A})
                    =\text{row rank}(A) \).
      \end{exparts} 
    \end{answer}
  \item  
    \cite{Monthly55p721}
    Prove
    (where \( A \) is an \( \nbyn{n} \) matrix and so defines
    a transformation of any \( n \)-dimensional space \( V \)
    with respect to \( B,B \) where \( B \) is a basis) that
    \( \dim(\rangespace{A}\intersection\nullspace{A})
           =\dim(\rangespace{A})-\dim(\rangespace{A^2}) \).
    Conclude
    \begin{exparts}
      \partsitem \( \nullspace{A}\subset\rangespace{A} \)
            iff
            \( \dim(\nullspace{A})=\dim(\rangespace{A})
                                   -\dim(\rangespace{A^2}) \);
      \partsitem \( \rangespace{A}\subseteq\nullspace{A} \)
            iff
            \( A^2=0 \);
      \partsitem \( \rangespace{A}=\nullspace{A} \)
            iff
            \( A^2=0 \)
            and \( \dim(\nullspace{A})=\dim(\rangespace{A}) \) ;
      \partsitem \( \dim(\rangespace{A}\intersection\nullspace{A})=0 \)
            iff
            \( \dim(\rangespace{A})=\dim(\rangespace{A^2}) \) ;
      \partsitem \textit{(Requires the Direct Sum subsection, 
            which is optional.)}
            \( V=\rangespace{A}\directsum\nullspace{A} \)
            iff
            \( \dim(\rangespace{A})=\dim(\rangespace{A^2}) \).
    \end{exparts}
    \begin{answer}
      \answerasgiven
      Let \( \sequence{\vec{z}_1,\dots,\vec{z}_k} \) be a basis for
      \( \rangespace{A}\intersection\nullspace{A} \)
      (\( k \) might be \( 0 \)).
      Let \( \vec{x}_1,\dots,\vec{x}_k\in V \) be such that
      \( A\vec{x}_i=\vec{z}_i \).
      Note \( \set{A\vec{x}_1,\dots,A\vec{x}_k} \) is linearly independent,
      and extend to a basis for \( \rangespace{A} \):
      \( A\vec{x}_1,\ldots,A\vec{x}_k,A\vec{x}_{k+1},\dots,A\vec{x}_{r_1} \)
      where \( r_1=\dim(\rangespace{A}) \).

      Now take \( \vec{x}\in V \).
      Write
      \begin{equation*}
        A\vec{x}=a_1(A\vec{x}_1)+\dots+a_{r_1}(A\vec{x}_{r_1})
      \end{equation*}
      and so
      \begin{equation*}
        A^2\vec{x}=a_1(A^2\vec{x}_1)+\dots+a_{r_1}(A^2\vec{x}_{r_1}).
      \end{equation*}
      But \( A\vec{x}_1,\dots,A\vec{x}_k\in\nullspace{A} \), so
      \( A^2\vec{x}_1=\zero,\dots,A^2\vec{x}_k=\zero \) and we now know
      \begin{equation*}
        A^2\vec{x}_{k+1},\dots,A^2\vec{x}_{r_1}
      \end{equation*}
      spans \( \rangespace{A^2} \).

      To see \( \set{A^2\vec{x}_{k+1},\dots,A^2\vec{x}_{r_1}} \)
      is linearly independent, write
      \begin{align*}
        b_{k+1}A^2\vec{x}_{k+1}+\dots+b_{r_1}A^2\vec{x}_{r_1}
        &=\zero                                                 \\
        A[b_{k+1}A\vec{x}_{k+1}+\dots+b_{r_1}A\vec{x}_{r_1}]
        &=\zero
      \end{align*}
      and, since
      \( b_{k+1}A\vec{x}_{k+1}+\dots+b_{r_1}A\vec{x}_{r_1}\in\nullspace{A} \)
      we get a contradiction unless it is \( \zero \) (clearly it is in
      \( \rangespace{A} \), but \( A\vec{x}_1,\ldots,A\vec{x}_k \) is a basis
      for \( \rangespace{A}\intersection\nullspace{A} \)).

      Hence \( \dim(\rangespace{A^2})=r_1-k=\dim(\rangespace{A})-
                \dim(\rangespace{A}\intersection\nullspace{A}) \).  
    \end{answer}
\end{exercises}





\begin{exercises}
  \item 
    Supply the intermediate steps in 
    \nearbyexample{exam:ThreeByThreeMatInv}.
    \begin{answer}
      Here is one way to proceed.
      Follow
      \begin{equation*}
       \grstep{\rho_1\leftrightarrow\rho_2}
       \begin{pmat}{rrr|rrr}
              1  &0  &1   &0  &1  &0  \\
              0  &3  &-1  &1  &0  &0  \\
              1  &-1 &0   &0  &0  &1
           \end{pmat}                          
       \grstep{-\rho_1+\rho_3}
       \begin{pmat}{rrr|rrr}
              1  &0  &1   &0  &1  &0  \\
              0  &3  &-1  &1  &0  &0  \\
              0  &-1 &-1  &0  &-1 &1
           \end{pmat}                        
       \end{equation*}
       with    
       \begin{align*}
         &\grstep{(1/3)\rho_2+\rho_3}
         \begin{pmat}{rrr|rrr}
                1  &0  &1     &0   &1  &0  \\
                0  &3  &-1    &1   &0  &0  \\
                0  &0  &-4/3  &1/3 &-1 &1
             \end{pmat}                                           \\ 
         &\grstep[-(3/4)\rho_3]{(1/3)\rho_2}
         \begin{pmat}{rrr|rrr}
                1  &0  &1     &0    &1   &0    \\
                0  &1  &-1/3  &1/3  &0   &0    \\
                0  &0  &1     &-1/4 &3/4 &-3/4
             \end{pmat}                                  \\
         &\grstep[-\rho_3+\rho_1]{(1/3)\rho_3+\rho_2}
         \begin{pmat}{rrr|rrr}
                1  &0  &0     &1/4  &1/4 &3/4  \\
                0  &1  &0     &1/4  &1/4 &-1/4 \\
                0  &0  &1     &-1/4 &3/4 &-3/4
             \end{pmat}                         
    \end{align*} 
    and read the answer off of the right side.
   \end{answer}
 \recommended \item 
    Use \nearbycorollary{cor:TwoByTwoInv} to decide if each matrix 
    has an inverse.
    \begin{exparts*}
      \partsitem
        $\begin{mat}[r]
           2  &1  \\
          -1  &1          
        \end{mat}$
      \partsitem
        $\begin{mat}[r]
          0  &4  \\
          1  &-3
        \end{mat}$
      \partsitem
        $\begin{mat}[r]
          2  &-3  \\
         -4  &6
        \end{mat}$
    \end{exparts*}
    \begin{answer}
      \begin{exparts*}
        \partsitem Yes, it has an inverse: $ad-bc=2\cdot 1-1\cdot(-1)\neq 0$.
        \partsitem Yes.
        \partsitem No.
      \end{exparts*}
    \end{answer}
  \recommended \item  
     For each invertible matrix in the prior problem, use
     \nearbycorollary{cor:TwoByTwoInv} to find its inverse.
     \begin{answer}
       \begin{exparts}
         \partsitem 
           $\displaystyle \frac{1}{2\cdot 1-1\cdot (-1)}
            \cdot\begin{mat}[r]
             1  &-1  \\
             1  &2
           \end{mat}
          =\frac{\displaystyle 1}{\displaystyle 3}\cdot\begin{mat}[r]
             1  &-1  \\
             1  &2
           \end{mat}
          =\begin{mat}[r]
             1/3  &-1/3  \\
             1/3  &2/3
           \end{mat}$
         \partsitem 
           $\displaystyle \frac{1}{0\cdot (-3)-4\cdot 1}
           \cdot\begin{mat}[r]
             -3  &-4  \\
             -1  &0
           \end{mat}
           =\begin{mat}[r]
             3/4  &1  \\
             1/4  &0
           \end{mat}$
         \partsitem The prior question shows that no inverse exists.
       \end{exparts}
     \end{answer}
  \recommended \item
    Find the inverse, if it exists, by using the Gauss-Jordan Method.
    Check the answers for the $\nbyn{2}$ matrices 
    with \nearbycorollary{cor:TwoByTwoInv}.
    \begin{exparts*}
      \partsitem $\begin{mat}[r]
                   3  &1  \\
                   0  &2
                 \end{mat}$
      \partsitem \( \begin{mat}[r]
                 2   &1/2  \\
                 3   &1
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                 2   &-4   \\
                -1   &2
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                 1   &1  &3  \\
                 0   &2  &4  \\
                 -1  &1  &0
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                 0   &1  &5  \\
                 0   &-2 &4  \\
                 2   &3  &-2
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                 2   &2  &3  \\
                 1   &-2 &-3 \\
                 4   &-2 &-3
               \end{mat} \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem The reduction is routine. 
          \begin{align*}
            \begin{pmat}{rr|rr}
              3  &1  &1  &0 \\
              0  &2  &0  &1
            \end{pmat}
            &\grstep[(1/2)\rho_2]{(1/3)\rho_1}
            \begin{pmat}{rr|rr}
              1  &1/3  &1/3  &0   \\
              0  &1    &0    &1/2
            \end{pmat}                                 \\
            &\grstep{-(1/3)\rho_2+\rho_1}
            \begin{pmat}{rr|rr}
              1  &0    &1/3  &-1/6 \\
              0  &1    &0    &1/2
            \end{pmat}
          \end{align*}
          This answer agrees with the answer from the check.
          \begin{equation*}
            \begin{mat}[r]
              3  &1  \\
              0  &2
            \end{mat}^{-1}
            =\frac{1}{3\cdot 2-0\cdot 1}\cdot
            \begin{mat}[r]
              2  &-1  \\
              0  &3
            \end{mat}
            =\frac{1}{6}\cdot
            \begin{mat}[r]
              2  &-1  \\
              0  &3
            \end{mat}
          \end{equation*}
        \partsitem This reduction is easy.
          \begin{multline*}
            \begin{pmat}{rr|rr}
              2  &1/2  &1  &0  \\
              3  &1    &0  &1
            \end{pmat}
            \grstep{-(3/2)\rho_1+\rho_2}
            \begin{pmat}{rr|rr}
              2  &1/2  &1     &0  \\
              0  &1/4  &-3/2  &1
            \end{pmat}                       \\
            \grstep[4\rho_2]{(1/2)\rho_1}
            \begin{pmat}{rr|rr}
              1  &1/4   &1/2   &0  \\
              0  &1     &-6    &4
            \end{pmat}                         
            \grstep{-(1/4)\rho_2+\rho_1}
            \begin{pmat}{rr|rr}
              1  &0     &2     &-1 \\
              0  &1     &-6    &4
            \end{pmat}
          \end{multline*}
          The check agrees.
          \begin{equation*}
            \frac{1}{2\cdot 1-3\cdot (1/2)}\cdot
            \begin{mat}[r]
              1  &-1/2  \\
              -3 &2
            \end{mat}
            =2\cdot
            \begin{mat}[r]
              1  &-1/2  \\
              -3 &2
            \end{mat}
          \end{equation*}
        \partsitem Trying the Gauss-Jordan reduction
          \begin{equation*}
            \begin{pmat}{rr|rr}
              2  &-4  &1  &0  \\
             -1  &2   &0  &1 
            \end{pmat}
            \grstep{(1/2)\rho_1+\rho_2}
            \begin{pmat}{rr|rr}
              2  &-4  &1   &0  \\
              0  &0   &1/2 &1 
            \end{pmat}
          \end{equation*}
          shows that the left side won't reduce to the identity, so no inverse
          exists.
          The check $ad-bc=2\cdot 2-(-4)\cdot (-1)=0$ agrees.
        \partsitem This produces an inverse.
          \begin{multline*}
            \begin{pmat}{rrr|rrr}
              1  &1  &3  &1  &0  &0  \\ 
              0  &2  &4  &0  &1  &0  \\
             -1  &1  &0  &0  &0  &1 
            \end{pmat}  
            \grstep{\rho_1+\rho_3}
            \begin{pmat}{rrr|rrr}
              1  &1  &3  &1  &0  &0  \\ 
              0  &2  &4  &0  &1  &0  \\
              0  &2  &3  &1  &0  &1 
            \end{pmat}                                             \\
           \begin{aligned}
            &\grstep{-\rho_2+\rho_3}
            \begin{pmat}{rrr|rrr}
              1  &1  &3  &1  &0  &0  \\ 
              0  &2  &4  &0  &1  &0  \\
              0  &0  &-1 &1  &-1 &1 
            \end{pmat}             
           \grstep[-\rho_3]{(1/2)\rho_2}
            \begin{pmat}{rrr|rrr}
              1  &1  &3  &1  &0   &0  \\ 
              0  &1  &2  &0  &1/2 &0  \\
              0  &0  &1  &-1 &1   &-1
            \end{pmat}                                        \\
            &\grstep[-3\rho_3+\rho_1]{-2\rho_3+\rho_2}
            \begin{pmat}{rrr|rrr}
              1  &1  &0  &4  &-3   &3  \\ 
              0  &1  &0  &2  &-3/2 &2  \\
              0  &0  &1  &-1 &1    &-1
            \end{pmat}                                         \\
            &\grstep{-\rho_2+\rho_1}
            \begin{pmat}{rrr|rrr}
              1  &0  &0  &2  &-3/2 &1  \\ 
              0  &1  &0  &2  &-3/2 &2  \\
              0  &0  &1  &-1 &1    &-1
            \end{pmat}
           \end{aligned} 
          \end{multline*}
       \partsitem This is one way to do the reduction.
          \begin{multline*}
            \begin{pmat}{rrr|rrr}
              0  &1  &5  &1  &0  &0  \\
              0  &-2 &4  &0  &1  &0  \\ 
              2  &3  &-2 &0  &0  &1
            \end{pmat}
            \grstep{\rho_3\leftrightarrow\rho_1}\;
            \begin{pmat}{rrr|rrr}
              2  &3  &-2 &0  &0  &1  \\
              0  &-2 &4  &0  &1  &0  \\ 
              0  &1  &5  &1  &0  &0  
            \end{pmat}                                            \\
            \begin{aligned}
              &\grstep{(1/2)\rho_2+\rho_3}
              \begin{pmat}{rrr|rrr}
                2  &3  &-2 &0  &0   &1  \\
                0  &-2 &4  &0  &1   &0  \\ 
                0  &0  &7  &1  &1/2 &0  
              \end{pmat}                                             \\
              &\grstep[-(1/2)\rho_2 \\ (1/7)\rho_3]{(1/2)\rho_1}
              \begin{pmat}{rrr|rrr}
                1  &3/2  &-1 &0    &0     &1/2  \\
                0  &1    &-2 &0    &-1/2  &0    \\ 
                0  &0    &1  &1/7  &1/14  &0  
              \end{pmat}                                   \\
              &\grstep[\rho_3+\rho_1]{2\rho_3+\rho_2}
              \begin{pmat}{rrr|rrr}
                1  &3/2  &0  &1/7  &1/14  &1/2  \\
                0  &1    &0  &2/7  &-5/14 &0    \\ 
                0  &0    &1  &1/7  &1/14  &0  
              \end{pmat}                                   \\
              &\grstep{-(3/2)\rho_2+\rho_1}
              \begin{pmat}{rrr|rrr}
                1  &0    &0  &-2/7 &17/28 &1/2  \\
                0  &1    &0  &2/7  &-5/14 &0    \\ 
                0  &0    &1  &1/7  &1/14  &0  
              \end{pmat}
            \end{aligned}
          \end{multline*}
        \partsitem There is no inverse.
          \begin{align*}
            \begin{pmat}{rrr|rrr}
              2  &2  &3  &1  &0  &0  \\
              1  &-2 &-3 &0  &1  &0  \\
              4  &-2 &-3 &0  &0  &1
            \end{pmat}
            &\grstep[-2\rho_1+\rho_3]{-(1/2)\rho_1+\rho_2}
            \begin{pmat}{rrr|rrr}
              2  &2  &3    &1     &0  &0  \\
              0  &-3 &-9/2 &-1/2  &1  &0  \\
              0  &-6 &-9   &-2    &0  &1
            \end{pmat}                                          \\
            &\grstep{-2\rho_2+\rho_3}
            \begin{pmat}{rrr|rrr}
              2  &2  &3    &1     &0   &0  \\
              0  &-3 &-9/2 &-1/2  &1   &0  \\
              0  &0  &0    &-1    &-2  &1
            \end{pmat}
          \end{align*}
          As a check, 
          note that the third column of the starting matrix is $3/2$ times 
          the second, and so it is indeed singular and therefore has no
          inverse.
      \end{exparts}
    \end{answer}
  \recommended \item 
    What matrix has this one for its inverse?
    \begin{equation*}
      \begin{mat}[r]
        1  &3  \\
        2  &5
       \end{mat}
     \end{equation*}
     \begin{answer}
       We can use \nearbycorollary{cor:TwoByTwoInv}. 
       \begin{equation*}
         \frac{1}{1\cdot 5-2\cdot 3}\cdot
         \begin{mat}[r]
           5  &-3  \\
          -2  &1
         \end{mat}
         =\begin{mat}[r]
           -5  &3  \\
            2  &-1
         \end{mat}
       \end{equation*}  
      \end{answer}
  \item 
   How does the inverse operation interact with scalar multiplication 
   and addition of matrices? 
   \begin{exparts}
      \partsitem What is the inverse of \( rH \)?
      \partsitem Is \( (H+G)^{-1}=H^{-1}+G^{-1} \)?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem The proof that the inverse is 
          \( r^{-1}H^{-1}=(1/r)\cdot H^{-1} \) 
          (provided, of course, that the matrix is invertible) is easy.
        \partsitem No.
          For one thing, the fact that $H+G$ has an inverse doesn't imply that
          $H$ has an inverse or that $G$ has an inverse.
          Neither of these matrices is invertible but their sum is.
          \begin{equation*}
            \begin{mat}[r]
              1  &0  \\
              0  &0
            \end{mat}
            \qquad
            \begin{mat}[r]
              0  &0  \\
              0  &1
            \end{mat}
          \end{equation*}
          Another point is that just because $H$ and $G$ each has an inverse
          doesn't mean $H+G$ has an inverse; here is an example.
          \begin{equation*}
            \begin{mat}[r]
              1  &0  \\
              0  &1
            \end{mat}
            \qquad
            \begin{mat}[r]
              -1  &0  \\
               0  &-1
            \end{mat}
          \end{equation*}
          Still a third point is that, even if the two matrices have inverses,
          and the sum has an inverse, doesn't imply that the equation holds:
          \begin{equation*}
            \begin{mat}[r]
              2  &0  \\
              0  &2
            \end{mat}^{-1}
            =\begin{mat}[r]
              1/2  &0  \\
              0    &1/2
            \end{mat}^{-1}
            \qquad
            \begin{mat}[r]
              3  &0  \\
              0  &3
            \end{mat}^{-1}
            =\begin{mat}[r]
              1/3  &0  \\
              0    &1/3
            \end{mat}^{-1}
          \end{equation*}
          but
          \begin{equation*}
            \begin{mat}[r]
              5  &0  \\
              0  &5
            \end{mat}^{-1}
            =\begin{mat}[r]
              1/5  &0  \\
              0    &1/5
            \end{mat}^{-1}
          \end{equation*}
          and $(1/2)_+(1/3)$ does not equal $1/5$.
      \end{exparts}  
    \end{answer}
  \recommended \item 
    Is \( (T^k)^{-1}=(T^{-1})^k \)?
    \begin{answer}
      Yes:
      \( T^k(T^{-1})^k=(TT\cdots T)\cdot (T^{-1}T^{-1}\cdots T^{-1})
         =T^{k-1}(TT^{-1})(T^{-1})^{k-1}=\dots=I \). 
    \end{answer}
  \item 
    Is \( H^{-1} \) invertible?
    \begin{answer}
       Yes, the inverse of \( H^{-1} \) is \( H \).  
    \end{answer}
  \item 
    For each real number \( \theta \) let
    \( \map{t_\theta}{\Re^2}{\Re^2} \) be represented with respect to the
    standard bases by this matrix.
    \begin{equation*}
      \begin{mat}
         \cos\theta  &-\sin\theta  \\
         \sin\theta  &\cos\theta
      \end{mat}
    \end{equation*}
    Show that \( t_{\theta_1+\theta_2}=t_{\theta_1}\cdot t_{\theta_2} \).
    Show also that \( {t_{\theta}}^{-1}=t_{-\theta} \).
    \begin{answer}
      One way to check that the first is true is with
      the angle sum formulas from trigonometry.
      \begin{multline*}
        \begin{mat}
          \cos(\theta_1+\theta_2) &-\sin(\theta_1+\theta_2)  \\
          \sin(\theta_1+\theta_2) &\cos(\theta_1+\theta_2)
        \end{mat}                                                 \\
        \begin{aligned}
        &=\begin{mat}
          \cos\theta_1\cos\theta_2-\sin\theta_1\sin\theta_2
            &-\sin\theta_1\cos\theta_2-\cos\theta_1\sin\theta_2  \\
          \sin\theta_1\cos\theta_2+\cos\theta_1\sin\theta_2
            &\cos\theta_1\cos\theta_2-\sin\theta_1\sin\theta_2
        \end{mat}                                                    \\
        &=\begin{mat}
          \cos\theta_1 &-\sin\theta_1  \\
          \sin\theta_1 &\cos\theta_1
        \end{mat}
        \begin{mat}
          \cos\theta_2 &-\sin\theta_2  \\
          \sin\theta_2 &\cos\theta_2
        \end{mat}
        \end{aligned}
      \end{multline*}
      Checking the second equation in this way is similar.

      Of course, the equations can be not just checked but also understood
      by recalling that $t_\theta$ is the map that rotates 
      vectors about the origin through an angle of \( \theta \)~radians.  
    \end{answer}
  \item \label{exer:TwoByTwoInv}
    Do the calculations for the proof of \nearbycorollary{cor:TwoByTwoInv}.
    \begin{answer}
      There are two cases.
      For the first case we assume that \( a \) is nonzero.
      Then
      \begin{equation*}
        \grstep{-(c/a)\rho_1+\rho_2}
        \begin{pmat}{cc|cc}
           a   &b           &1     &0   \\
           0   &-(bc/a)+d   &-c/a  &1
         \end{pmat}
        =\begin{pmat}{cc|cc}
           a   &b           &1     &0   \\
           0   &(ad-bc)/a   &-c/a  &1
         \end{pmat}
      \end{equation*}
      shows that the matrix is invertible (in this \( a\neq 0 \) case) 
      if and only if  \( ad-bc\neq 0 \).
      To find the inverse, we finish with the Jordan half of the reduction.
      \begin{align*}
        &\grstep[(a/ad-bc)\rho_2]{(1/a)\rho_1}
        \begin{pmat}{cc|cc}
           1   &b/a     &1/a         &0   \\
           0   &1       &-c/(ad-bc)  &a/(ad-bc)
         \end{pmat}                                       \\   
        &\grstep{-(b/a)\rho_2+\rho_1}
        \begin{pmat}{cc|cc}
           1   &0       &d/(ad-bc)   &-b/(ad-bc)   \\
           0   &1       &-c/(ad-bc)  &a/(ad-bc)
         \end{pmat}                                
      \end{align*}

      The other case is the \( a=0 \) case.
      We swap to get $c$ into the $1,1$ position.
      \begin{equation*}
        \grstep{\rho_1\leftrightarrow\rho_2}
        \begin{pmat}{cc|cc}
          c  &d  &0  &1  \\
          0  &b  &1  &0
        \end{pmat}
      \end{equation*}
      This matrix is nonsingular if and only if
      both \( b \) and \( c \) are nonzero
      (which, under the case assumption that \( a=0 \), 
      holds if and only if \( ad-bc\neq 0 \)).
      To find the inverse
      we do the Jordan half.
      \begin{equation*}
        \grstep[(1/b)\rho_2]{(1/c)\rho_1}
        \begin{pmat}{cc|cc}
          1  &d/c  &0       &1/c  \\
          0  &1    &1/b     &0
        \end{pmat}                        
        \grstep{-(d/c)\rho_2+\rho_1}
        \begin{pmat}{cc|cc}
          1  &0  &-d/bc  &1/c  \\
          0  &1  &1/b    &0
        \end{pmat}
      \end{equation*}
      (Note that this is what is required, since \( a=0 \) gives that
      \( ad-bc=-bc \)).
    \end{answer}
  \item 
    Show that this matrix 
    \begin{equation*}
      H=\begin{mat}[r]
          1  &0   &1  \\
          0  &1   &0
        \end{mat}
    \end{equation*}
    has infinitely many right inverses.
    Show also that it has no left inverse.
    \begin{answer}
      With $H$ a $\nbym{2}{3}$ matrix,
      in looking for a matrix $G$ such that the combination $HG$
      acts as the $\nbyn{2}$ identity we
      need $G$ to be $\nbym{3}{2}$. 
      Setting up the equation
      \begin{equation*}
          \begin{mat}[r]
             1  &0   &1  \\
             0  &1   &0
           \end{mat}
          \begin{mat}
             m  &n  \\
             p  &q  \\
             r  &s
          \end{mat}
        =
          \begin{mat}[r]
             1  &0  \\
             0  &1
          \end{mat}
      \end{equation*}
      and solving the resulting linear system
      \begin{equation*}
        \begin{linsys}{6}
           m  &   &   &   &+r &   &=    &1  \\
              &n  &   &   &   &+s &=    &0  \\
              &   &p  &   &   &   &=    &0  \\
              &   &   &q  &   &   &=    &1
         \end{linsys}
      \end{equation*}
      gives infinitely many solutions.
      \begin{equation*}
        \set{\colvec{m \\ n \\ p \\ q \\ r \\ s}
              =\colvec[r]{1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0}
              +r\cdot \colvec[r]{-1 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0}
              +s\cdot \colvec[r]{0 \\ -1 \\ 0 \\ 0 \\ 0 \\ 1}
             \suchthat r,s\in\Re}
      \end{equation*}
      Thus $H$ has infinitely many right inverses.

      As for left inverses, the equation
      \begin{equation*}
         \begin{mat}
            a  &b  \\
            c  &d
         \end{mat}
         \begin{mat}[r]
             1  &0   &1  \\
             0  &1   &0
          \end{mat}
          =\begin{mat}[r]
             1  &0  &0  \\
             0  &1  &0  \\
             0  &0  &1
           \end{mat}
      \end{equation*}
      gives rise to a linear system with nine equations and four unknowns.
      \begin{equation*}
        \begin{linsys}{6}
          a & & & & & & & & & & &= &1 \\
            & &b& & & & & & & & &= &0 \\
          a & & & & & & & & & & &= &0 \\
            & & & &c& & & & & & &= &0 \\
            & & & & & &d& & & & &= &1 \\
            & & & &c& & & & & & &= &0 \\
            & & & & & & & &e& & &= &0 \\
            & & & & & & & & & &f&= &0 \\
            & & & & & & & &e& & &= &1 
        \end{linsys}
      \end{equation*}
      This system is inconsistent (the first equation conflicts
      with the third, as do the seventh and ninth) 
      and so there is no left inverse.
    \end{answer}
  \item 
    In the review of inverses example, % \nearbyexample{ex:ProjLeftInvOfEmbed},
    starting this subsection, how many left inverses has \( \iota \)?
    \begin{answer}
      With respect to the standard bases we have
      \begin{equation*}
        \rep{\iota}{\stdbasis_2,\stdbasis_3}
        =\begin{mat}[r]
          1  &0  \\
          0  &1  \\
          0  &0 
        \end{mat}
      \end{equation*}
      and setting up the equation to find the matrix inverse
      \begin{equation*}
        \begin{mat}
          a &b &c \\
          d &e &f
        \end{mat}
        \begin{mat}[r]
          1 &0  \\
          0 &1  \\
          0 &0
        \end{mat}
        =\begin{mat}[r]
          1 &0  \\
          0 &1 
        \end{mat}
        =\rep{\identity}{\stdbasis_2,\stdbasis_2}
      \end{equation*}
      gives rise to a linear system.
      \begin{equation*}
        \begin{linsys}{6}
          a & & & & & & & & & & &= &1 \\
            & &b& & & & & & & & &= &0 \\
            & & & & & &d& & & & &= &0 \\
            & & & & & & & &e& & &= &1 
        \end{linsys}
      \end{equation*}
      There are infinitely many solutions in $a,\ldots,f$ 
      to this system because two of these variables are entirely unrestricted
      \begin{equation*}
        \set{\colvec{a \\ b \\ c \\ d \\ e \\ f}
             =\colvec[r]{1 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0}
              +c\cdot \colvec[r]{0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0}
              +f\cdot \colvec[r]{0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1}
             \suchthat c,f\in\Re}
      \end{equation*}
      and so there are infinitely many solutions to the matrix equation.
      \begin{equation*}
        \set{\begin{mat}
               1  &0  &c  \\
               0  &1  &f
             \end{mat}
             \suchthat c,f\in\Re}
      \end{equation*}
      With the bases still fixed at $\stdbasis_2,\stdbasis_2$, 
      for instance taking $c=2$ and 
      $f=3$ gives a matrix representing this map.
      \begin{equation*}
        \colvec{x \\ y \\ z}
        \;\mapsunder{f_{2,3}}\;
        \colvec{x+2z \\ y+3z}
      \end{equation*}
      The check that $\composed{f_{2,3}}{\iota}$ is the identity map on
      $\Re^2$ is easy.
    \end{answer}
  \item  
    If a matrix has infinitely many right-inverses, can it have infinitely
    many left-inverses?
    Must it have?
    \begin{answer}
      By \nearbylemma{le:LeftAndRightInvEqual} it cannot have infinitely many 
      left inverses, because
      a matrix with both left and right inverses has only one of each (and
      that one of each is one of both\Dash the left and right inverse matrices
      are equal).  
    \end{answer}
  \item Assume that $\map{g}{V}{W}$ is linear.
    One of these is true, the other is false.
    Which is which?
    \begin{exparts}
      \partsitem If $\map{f}{W}{V}$ is a left inverse of $g$ then $f$
        must be linear.
      \partsitem If $\map{f}{W}{V}$ is a right inverse of $g$ then $f$
        must be linear.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem True,
          It must be linear, 
          as the proof from Theorem~II.\ref{th:OOHomoEquivalence} shows.
        \partsitem False.
          It may be linear, but it need not be.
          Consider the projection map $\map{\pi}{\Re^3}{\Re^2}$ described
          at the start of this subsection.
          Define $\map{\eta}{\Re^2}{\Re^3}$ in this way.
          \begin{equation*}
            \colvec{x  \\  y}\mapsto\colvec{x  \\ y \\ 1}
          \end{equation*}
          It is a right inverse of $\pi$ because $\composed{\pi}{\eta}$
          does this.  
          \begin{equation*}
            \colvec{x  \\  y}\mapsto\colvec{x  \\ y \\ 1}\mapsto\colvec{x \\ y}
          \end{equation*}
          It is not linear because it does not map the zero vector to the zero 
          vector.
      \end{exparts}
    \end{answer}
  \recommended \item
    Assume that \( H \) is invertible and that \( HG \) is the zero matrix.
    Show that \( G \) is a zero matrix.
    \begin{answer}
      The associativity of matrix multiplication gives
      \( H^{-1}(HG)=H^{-1}Z=Z \) and also
      \( H^{-1}(HG)=(H^{-1}H)G=IG=G \).
    \end{answer}
  \item 
    Prove that if \( H \) is invertible then
    the inverse commutes with a matrix \( GH^{-1}=H^{-1}G \) 
    if and only if $H$ itself commutes with that matrix \( GH=HG \).
    \begin{answer}
       Multiply both sides of the first equation by $H$.
    \end{answer}
  \recommended \item
    Show that if \( T \) is square and if \( T^4 \) is the zero matrix
    then \( (I-T)^{-1}=I+T+T^2+T^3 \).
    Generalize.
    \begin{answer}
      Checking that when $I-T$ is multiplied on both sides by that expression
      (assuming that $T^4$ is the zero matrix) then the result is the 
      identity matrix is easy.
      The obvious generalization is that if \( T^n \) is the zero matrix
      then \( (I-T)^{-1}=I+T+T^2+\cdots+T^{n-1} \); the check again is
      easy.  
    \end{answer}
  \recommended \item 
    Let \( D \) be diagonal.
    Describe \( D^2 \), \( D^3 \), \ldots\thinspace, etc.
    Describe \( D^{-1} \), \( D^{-2} \), \ldots\thinspace, etc.
    Define \( D^0 \) appropriately.
    \begin{answer}
      The powers of the matrix are formed by taking the powers of the
      diagonal entries.
      That is,  \( D^2 \) is all zeros except for diagonal entries of
      \( {d_{1,1}}^2 \), \( {d_{2,2}}^2 \), etc.
      This suggests defining \( D^0 \) to be the identity matrix.  
    \end{answer}
  \item
    Prove that any matrix row-equivalent to an invertible matrix is also
    invertible.
    \begin{answer}
      Assume that $B$ is row equivalent to $A$ and that $A$ is invertible.
      Because they are row-equivalent, there is a sequence of row steps 
      to reduce one to the other.
      We can do that reduction with matrices, for instance, $A$ can 
      change by row operations to $B$ as $B=R_n\cdots R_1A$.
      This equation gives $B$ as a product of invertible matrices and
      by \nearbylemma{lem:ProdInvIsInv} then, $B$ is also invertible.  
    \end{answer}
  \item 
    \textit{The first question below appeared as
    \nearbyexercise{exer:RankProdLeqRankFacts}.}
    \begin{exparts}
      \partsitem Show that the rank of the product of two matrices is less than
        or equal to the minimum of the rank of each.
      \partsitem Show that if \( T \)  and \( S \) are square then \( TS=I \)
         if and only if \( ST=I \).
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem See the answer to
          \nearbyexercise{exer:RankProdLeqRankFacts}.
        \partsitem We will show that both conditions are equivalent to
          the condition that the two matrices be nonsingular. 

          As \( T \) and \( S \) are square and their product is defined,
          they are equal-sized, say \( \nbyn{n} \).
          Consider the \( TS=I \) half.
          By the prior item the rank of \( I \) is less than or equal to
          the minimum of the rank of \( T \) and the rank of \( S \).
          But the rank of \( I \) is \( n \), so the rank of \( T \) and
          the rank of \( S \) must each be \( n \).
          Hence each is nonsingular.

          The same argument shows that \( ST=I \) implies that 
          each is nonsingular.
      \end{exparts}  
    \end{answer}
  \item 
    Show that the inverse of a permutation matrix is its transpose.
    \begin{answer}
      Inverses are unique, so we need only show that it works.
      The check appears above as
      \nearbyexercise{exer:PermTimesTransEqId}.  
    \end{answer}
  \item 
    \textit{The first two parts of this question appeared as
    \nearbyexercise{exer:TranspAndMult}.}
    \begin{exparts}
      \partsitem Show that \( \trans{(GH)}=\trans{H}\trans{G} \).
      \partsitem A square matrix is {\em symmetric\/} if each 
        \( i,j \) entry equals the
        \( j,i \) entry (that is, if the matrix equals its transpose).
        Show that 
        the matrices \( H\trans{H} \) and \( \trans{H}H \) are symmetric.
      \partsitem Show that the inverse of the transpose is the transpose 
        of the inverse.
      \partsitem Show that the inverse of a symmetric matrix is symmetric.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem See the answer for \nearbyexercise{exer:TranspAndMult}.
        \partsitem See the answer for \nearbyexercise{exer:TranspAndMult}.
        \partsitem Apply the first part to  
          \( I=AA^{-1} \) to get
          $I=\trans{I}=\trans{(AA^{-1})}=\trans{(A^{-1})}\trans{A}$.
        \partsitem Apply the prior item with \( \trans{A}=A \),
          as \( A \) is symmetric.
      \end{exparts}  
    \end{answer}
  \recommended \item
    \textit{The items starting this question appeared as
    \nearbyexercise{exer:ZeroDivisor}.}
    \begin{exparts}
      \partsitem Prove that the composition of the projections
        \( \map{\pi_x,\pi_y}{\Re^3}{\Re^3} \) is the zero map despite that
        neither is the zero map.
      \partsitem Prove that the composition of the derivatives
        \( \map{d^2/dx^2,\,d^3/dx^3}{\polyspace_4}{\polyspace_4} \)
        is the zero map despite that neither map is the zero map.
      \partsitem Give matrix equations representing each of the prior two
        items.
    \end{exparts}
    When two things multiply to give zero despite that neither is zero, each is
    said to be a \definend{zero divisor}.\index{zero division}
    Prove that no zero divisor is invertible.
    \begin{answer}
      For the answer to the items making up the first half, see 
      \nearbyexercise{exer:ZeroDivisor}.
      For the proof in the second half, assume that $A$ is a zero divisor so
      there is a nonzero matrix $B$ with $AB=Z$ 
      (or else $BA=Z$; this case is similar), 
      If $A$ is invertible
      then $A^{-1}(AB)=(A^{-1}A)B=IB=B$ but also
      $A^{-1}(AB)=A^{-1}Z=Z$, contradicting that $B$ is nonzero.
    \end{answer}
  \item 
    In real number algebra, there are exactly two numbers, $1$ and $-1$, 
    that are their own multiplicative inverse.
    Does \( H^2=I \) have exactly two solutions for \( \nbyn{2} \)
    matrices?
    \begin{answer}
      Here are four solutions to \( H^2=I \).
      \begin{equation*}
        \begin{mat}[r]
          \pm 1  &0  \\
          0      &\pm 1
        \end{mat}
      \end{equation*}   
    \end{answer}
  \item 
   Is the relation `is a two-sided inverse of' transitive?
   Reflexive?
   Symmetric?
   \begin{answer}
     It is not reflexive since, for instance, 
     \begin{equation*}
       H=\begin{mat}[r]
         1  &0  \\
         0  &2
       \end{mat}
     \end{equation*}
     is not a two-sided inverse of itself.
     The same example shows that it is not transitive.
     That matrix has this two-sided inverse
     \begin{equation*}
       G=\begin{mat}[r]
         1  &0  \\
         0  &1/2
       \end{mat}
     \end{equation*}
     and while \( H \) is a two-sided inverse of \( G \) and \( G \)
     is a two-sided inverse of \( H \), we know that \( H \) is not a two-sided
     inverse of \( H \).
     However, the relation is symmetric:~if \( G \) is a two-sided inverse of 
     \( H \) then
     \( GH=I=HG \) and therefore \( H \) is also a two-sided
     inverse of \( G \).  
   \end{answer}
  \item  
    \cite{Monthly51p614}
    Prove: if the sum of the elements of each row of a square
    matrix is \( k \), then the sum of the elements in each row of the
    inverse matrix is \( 1/k \).
    \begin{answer}
      \answerasgiven %
      Let \( A \) be \( \nbyn{m} \), non-singular, with the stated property.
      Let \( B \) be its inverse.
      Then for \( n\leq m \),
      \begin{equation*}
        1
        =\sum_{r=1}^{m}\delta_{nr}
        =\sum_{r=1}^{m}\sum_{s=1}^{m}b_{ns}a_{sr}
        =\sum_{s=1}^{m}\sum_{r=1}^{m}b_{ns}a_{sr}
        =k\sum_{s=1}^{m}b_{ns}
      \end{equation*}
      (\( A \) is singular if \( k=0 \)).  
   \end{answer}
\end{exercises}
\index{matrix!inverse|)}








  \puzzle \item 
    Show that
    \begin{equation*}
      F_n=
      \begin{vmat}
        1  &-1  &1  &-1  &1  &-1  &\ldots  \\
        1  &1   &0  &1   &0  &1   &\ldots  \\
        0  &1   &1  &0   &1  &0   &\ldots  \\
        0  &0   &1  &1   &0  &1   &\ldots  \\
        .  &.   &.  &.   &.  &.   &\ldots
      \end{vmat}
    \end{equation*}
    where \( F_n \) is the \( n \)-th term of
    \( 1,1,2,3,5,\dots,x,y,x+y,\ldots\, \), the Fibonacci sequence,
    and the determinant is of order \( n-1 \).
    \cite{Monthly49p409}
    \begin{answer}
      \answerasgiven %
      Denoting the above determinant by \( D_n \), it is seen that
      \( D_2=1 \), \( D_3=2 \).
      It remains to show that \( D_n=D_{n-1}+D_{n-2},\; n\geq 4 \).
      In \( D_n \) subtract the \( (n-3) \)-th column from the \( (n-1) \)-th,
      the \( (n-4) \)-th from the \( (n-2) \)-th, \ldots, the first from
      the third, obtaining
      \begin{equation*}
        F_n=
        \begin{vmat}
          1  &-1  &0  &0   &0  &0   &\ldots  \\
          1  &1   &-1 &0   &0  &0   &\ldots  \\
          0  &1   &1  &-1  &0  &0   &\ldots  \\
          0  &0   &1  &1   &-1 &0   &\ldots  \\
          .  &.   &.  &.   &.  &.   &\ldots
        \end{vmat}.
      \end{equation*}
      By expanding this determinant with reference to the first row, there
      results the desired relation.  
    \end{answer}

\end{exercises}
\index{Laplace determinant expansion|)}


    \begin{exparts}
        \partsitem \cite{Monthly55p249}
        Prove that the area of a triangle with vertices \( (x_1,y_1) \),
        \( (x_2,y_2) \), and \( (x_3,y_3) \) is
        \begin{equation*}
          \frac{1}{2}
          \begin{vmat}
            x_1  &x_2 &x_3  \\
            y_1  &y_2 &y_3  \\
            1    &1   &1
          \end{vmat}.
        \end{equation*}
      \partsitem \cite{MathMag73p286}
        Prove that the area of a triangle with vertices at \( (x_1,y_1) \),
        \( (x_2,y_2) \), and \( (x_3,y_3) \) whose coordinates are integers
        has an area of \( N \) or \( N/2 \) for some positive integer \( N \).
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        For geometric insight, this 
        picture shows that the box formed by the three vectors.
        Note that all 
        three vectors end in the $z=1$ plane.
        Below the two vectors on the right is the line through
        $(x_2,y_2)$ and $(x_3,y_3)$.
        \begin{center}
          \includegraphics{ch4.49}
        \end{center}
        The box will 
        have a nonzero volume unless the triangle formed by the ends of the
        three is degenerate.
        That only happens (assuming that $(x_2,y_3)\neq (x_3,y_3)$)
        if  $(x,y)$ lies on the line through the other two. 
       \partsitem \answerasgiven %
        We find the altitude through $(x_1,y_1)$ of a triangle with vertices
        $(x_1,y_1)$ $(x_2,y_2)$ and $(x_3,y_3)$ in the usual
        way from the normal form of the above:
        \begin{equation*}
          \frac{1}{\sqrt{(x_2-x_3)^2+(y_2-y_3)^2}}
          \begin{vmat}
            x_1  &x_2  &x_3  \\
            y_1  &y_2  &y_3  \\
            1    &1    &1
          \end{vmat}.
        \end{equation*}
        Another step shows the area of the triangle to be
        \begin{equation*}
          \frac{1}{2}
          \begin{vmat}
            x_1  &x_2  &x_3  \\
            y_1  &y_2  &y_3  \\
            1    &1    &1
          \end{vmat}.
        \end{equation*}
        This exposition reveals the \textit{modus operandi} more clearly
        than the usual proof of showing a collection of terms to be identical
        with the determinant.
       \partsitem  \answerasgiven %
        Let
        \begin{equation*}
          D=
          \begin{vmat}
            x_1  &x_2  &x_3  \\
            y_1  &y_2  &y_3  \\
            1    &1    &1
          \end{vmat}
        \end{equation*}
        then the area of the triangle is $(1/2)\deter{D}$.
        Now if the coordinates are all integers, then $D$ is an integer.
      \end{exparts}
    \end{answer}
\end{exercises}


   \item \label{exer:ThreeByThreeDetForm} 
    Do the Gaussian reduction to check
    the formula for $\nbyn{3}$ matrices stated in the preamble to
    this section.
    \begin{center}
      \( \begin{mat}
               a  &b  &c  \\
               d  &e  &f  \\
               g  &h  &i
         \end{mat} \)
      is nonsingular iff
      \( aei+bfg+cdh-hfa-idb-gec \neq 0 \)
    \end{center}
    \begin{answer}
      We first reduce the matrix to echelon form.
      To begin, assume that \( a\neq 0 \) and that \( ae-bd\neq 0 \).
      \begin{multline*}
        \grstep{(1/a)\rho_1}
        \begin{mat}
           1   &b/a   &c/a   \\
           d   &e     &f     \\
           g   &h     &i
         \end{mat}                                           
        \grstep[-g\rho_1+\rho_3]{-d\rho_1+\rho_2}
        \begin{mat}
           1   &b/a           &c/a           \\
           0   &(ae-bd)/a     &(af-cd)/a     \\
           0   &(ah-bg)/a     &(ai-cg)/a
         \end{mat}                                            \\
        \grstep{(a/(ae-bd))\rho_2}
        \begin{mat}
           1   &b/a           &c/a             \\
           0   &1             &(af-cd)/(ae-bd) \\
           0   &(ah-bg)/a     &(ai-cg)/a
         \end{mat}
      \end{multline*}
      This step finishes the calculation.
      \begin{equation*}
        \grstep{((ah-bg)/a)\rho_2+\rho_3}
        \begin{mat}
           1   &b/a    &c/a             \\
           0   &1      &(af-cd)/(ae-bd)      \\
           0   &0      &(aei+bgf+cdh-hfa-idb-gec)/(ae-bd)
         \end{mat}
      \end{equation*}
      Now assuming that $a\neq 0$ and \( ae-bd\neq 0 \), 
      the original matrix is nonsingular
      if and only if the \( 3,3 \) entry above is nonzero.
      That is, under the assumptions, the original matrix is
      nonsingular if and only if $aei+bgf+cdh-hfa-idb-gec\neq 0$,
      as required.

      We finish by running down what happens if the assumptions that were
      taken for convenience in the prior paragraph do not hold.
      First, if \( a\neq 0 \) but \( ae-bd=0 \) then we can swap
      \begin{equation*}
        \begin{mat}
           1   &b/a           &c/a           \\
           0   &0             &(af-cd)/a     \\
           0   &(ah-bg)/a     &(ai-cg)/a
         \end{mat}                                  
        \grstep{\rho_2\leftrightarrow\rho_3}
        \begin{mat}
           1   &b/a           &c/a           \\
           0   &(ah-bg)/a     &(ai-cg)/a     \\
           0   &0             &(af-cd)/a
         \end{mat}
      \end{equation*}
      and conclude that the matrix is nonsingular if and only if either
      \( ah-bg=0 \) or \( af-cd=0 \).
      The condition `\( ah-bg=0 \) or \( af-cd=0 \)' is equivalent to
      the condition `\( (ah-bg)(af-cd)=0 \)'.
      Multiplying out and using the case assumption that $ae-bd=0$
      to substitute $ae$ for $bd$ gives this.
      \begin{multline*}
         0=ahaf-ahcd-bgaf+bgcd
          =ahaf-ahcd-bgaf+aegc            \\
          =a(haf-hcd-bgf+egc)
      \end{multline*}
      Since \( a\neq 0 \), we have that the matrix
      is nonsingular if and only if \( haf-hcd-bgf+egc=0 \).
      Therefore, in this \( a\neq 0 \) and \( ae-bd=0 \) case, 
      the matrix is nonsingular when
      \( haf-hcd-bgf+egc-i(ae-bd)=0 \).

      The remaining cases are routine.
      Do the \( a=0 \) but \( d\neq 0 \) case and the \( a=0 \) and \( d=0 \)
      but \( g\neq 0 \) case by first swapping rows and then going on as
      above.
      The \( a=0 \), \( d=0 \), and \( g=0 \) case is easy\Dash that matrix is
      singular since the columns form a linearly dependent set, and the
      determinant comes out to be zero.  
    \end{answer}



  \recommended \item
    Prove that the area of this region in the plane
    \begin{center}
      \includegraphics{ch4.30}
    \end{center}
    is equal to the value of this determinant.
    \begin{equation*}
       \det(
       \begin{mat}
          x_1  &x_2  \\
          y_1  &y_2
       \end{mat})
    \end{equation*}
    Compare with this.
    \begin{equation*}
       \det(
       \begin{mat}
          x_2  &x_1  \\
          y_2  &y_1
       \end{mat})
    \end{equation*}
    \begin{answer}
      One way is to count these areas
      \begin{center}
        \includegraphics{ch4.31}
      \end{center}
      by taking the area of the entire rectangle and subtracting the area of
      $A$ the upper-left rectangle, $B$ the upper-middle triangle,
      $D$ the upper-right triangle, $C$ the lower-left triangle, 
      $E$ the lower-middle triangle, and $F$ the lower-right rectangle       
      \( (x_1+x_2)(y_1+y_2)-x_2y_1-(1/2)x_1y_1-(1/2)x_2y_2
              -(1/2)x_2y_2-(1/2)x_1y_1-x_2y_1 \).
      Simplification gives the determinant formula.

      This determinant is the negative of the one above; the formula
      distinguishes whether the second column is counterclockwise from
      the first.  
     \end{answer}





\recommended \item 
    Do the indicated vector operation, if it is defined.
    \begin{exparts*}
      \partsitem \( \colvec[r]{2 \\ 1 \\ 1}
               +\colvec[r]{3 \\ 0 \\ 4} \)
      \partsitem \( 5\colvec[r]{4 \\ -1} \)
      \partsitem \( \colvec[r]{1 \\ 5 \\ 1}
               -\colvec[r]{3 \\ 1 \\ 1} \)
      \partsitem \( 7\colvec[r]{2 \\ 1}
               +9\colvec[r]{3 \\ 5} \)
      \partsitem \( \colvec[r]{1 \\ 2}
               +\colvec[r]{1 \\ 2 \\ 3} \)
      \partsitem \( 6\colvec[r]{3 \\ 1 \\ 1}
               -4\colvec[r]{2 \\ 0 \\ 3}
               +2\colvec[r]{1 \\ 1 \\ 5} \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts*}
        \partsitem \( \colvec[r]{5 \\ 1 \\ 5} \)
        \partsitem \( \colvec[r]{20 \\ -5} \)
        \partsitem \( \colvec[r]{-2 \\ 4 \\ 0} \)
        \partsitem \( \colvec[r]{41 \\ 52} \)
        \partsitem Not defined.
        \partsitem \( \colvec[r]{12 \\ 8 \\ 4} \)
      \end{exparts*}  
     \end{answer}
  \recommended \item  \label{exer:SolveInMatrixNotation}
    Solve each system using matrix notation.
    Express the solution using vectors.
    \begin{exparts*}
      \partsitem \( \begin{linsys}[t]{2}
                  3x  &+  &6y  &=  &18  \\
                   x  &+  &2y  &=  &6   
                   \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{2}
                   x  &+  &y   &=  &1  \\
                   x  &-  &y   &=  &-1   
                    \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{3}
                   x_1  &   &     &+  &x_3   &=  &4  \\
                   x_1  &-  &x_2  &+  &2x_3  &=  &5  \\
                  4x_1  &-  &x_2  &+  &5x_3  &=  &17  
                   \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{3}
                   2a   &+  &b    &-  &c     &=  &2  \\
                   2a   &   &     &+  &c     &=  &3  \\
                    a   &-  &b    &   &      &=  &0   
                    \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{4}
                     x  &+  &2y   &-   &z   &    &    &=  &3  \\
                    2x  &+  &y    &    &    &+   &w   &=  &4  \\
                     x  &-  &y    &+   &z   &+   &w   &=  &1  
                    \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{4}
                     x  &   &     &+   &z   &+   &w   &=  &4  \\
                    2x  &+  &y    &    &    &-   &w   &=  &2  \\
                    3x  &+  &y    &+   &z   &    &    &=  &7  
                     \end{linsys}  \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem This reduction
          \begin{equation*}
            \begin{amat}[r]{2}
              3  &6  &18 \\
              1  &2  &6
            \end{amat}
            \grstep{(-1/3)\rho_1+\rho_2}
            \begin{amat}[r]{2}
              3  &6  &18 \\
              0  &0  &0
            \end{amat}
          \end{equation*}
          leaves \( x \) leading and \( y \) free.
          Making \( y \) the parameter, gives \( x=6-2y \) and this solution
          set.
          \begin{equation*}
            \set{\colvec[r]{6 \\ 0}+\colvec[r]{-2 \\ 1}y
              \suchthat y\in\Re}
          \end{equation*}
        \partsitem A reduction
          \begin{equation*}
            \begin{amat}[r]{2}
              1  &1  &1  \\
              1  &-1 &-1
            \end{amat}
            \grstep{-\rho_1+\rho_2}
            \begin{amat}[r]{2}
              1  &1  &1  \\
              0  &-2 &-2
            \end{amat}
          \end{equation*}
          gives the unique solution \( y=1 \), \( x=0 \).
          The solution set is this.
          \begin{equation*}
            \set{\colvec[r]{0 \\ 1} }
          \end{equation*}
        \partsitem Gauss's Method
          \begin{equation*}
            \begin{amat}[r]{3}
              1  &0  &1  &4  \\
              1  &-1 &2  &5  \\
              4  &-1 &5  &17
            \end{amat}
            \grstep[-4\rho_1+\rho_3]{-\rho_1+\rho_2}
            \begin{amat}[r]{3}
              1  &0  &1  &4  \\
              0  &-1 &1  &1  \\
              0  &-1 &1  &1
            \end{amat}     
            \grstep{-\rho_2+\rho_3}
            \begin{amat}[r]{3}
              1  &0  &1  &4  \\
              0  &-1 &1  &1  \\
              0  &0  &0  &0
            \end{amat}
          \end{equation*}
          leaves \( x_1 \) and \( x_2 \) leading with \( x_3 \) free.
          The solution set is this.
          \begin{equation*}
            \set{\colvec[r]{4 \\ -1 \\ 0}+\colvec[r]{-1 \\ 1 \\ 1}x_3
              \suchthat x_3\in\Re}
          \end{equation*}
        \partsitem This reduction
          \begin{align*}
            \begin{amat}[r]{3}
              2  &1  &-1 &2  \\
              2  &0  &1  &3  \\
              1  &-1 &0  &0
            \end{amat}
            &\grstep[-(1/2)\rho_1+\rho_3]{-\rho_1+\rho_2}
            \begin{amat}[r]{3}
              2  &1    &-1   &2  \\
              0  &-1   &2    &1  \\
              0  &-3/2 &1/2  &-1
            \end{amat}                                          \\
            &\grstep{(-3/2)\rho_2+\rho_3}
            \begin{amat}[r]{3}
              2  &1  &-1   &2  \\
              0  &-1 &2    &1  \\
              0  &0  &-5/2 &-5/2
            \end{amat}
          \end{align*}
          shows that the solution set is a singleton set.
          \begin{equation*}
            \set{\colvec[r]{1 \\ 1 \\ 1}}
          \end{equation*}
        \partsitem This reduction is easy
          \begin{align*}
            \begin{amat}[r]{4}
              1  &2  &-1 &0  &3 \\
              2  &1  &0  &1  &4 \\
              1  &-1 &1  &1  &1
            \end{amat}
            &\grstep[-\rho_1+\rho_3]{-2\rho_1+\rho_2}
            \begin{amat}[r]{4}
              1  &2  &-1 &0  &3  \\
              0  &-3 &2  &1  &-2 \\
              0  &-3 &2  &1  &-2
            \end{amat}                                       \\
            &\grstep{-\rho_2+\rho_3}
            \begin{amat}[r]{4}
              1  &2  &-1 &0  &3  \\
              0  &-3 &2  &1  &-2 \\
              0  &0  &0  &0  &0
            \end{amat}
          \end{align*}
          and ends with \( x \) and $y$ leading while \( z \) and \( w \) are
          free.
          Solving for \( y \) gives \( y=(2+2z+w)/3 \) and substitution shows
          that \( x+2(2+2z+w)/3-z=3 \) so \( x=(5/3)-(1/3)z-(2/3)w \),
          making this the solution set.
          \begin{equation*}
            \set{\colvec[r]{5/3 \\ 2/3 \\ 0 \\ 0}
                 +\colvec[r]{-1/3 \\ 2/3 \\ 1 \\ 0}z
                 +\colvec[r]{-2/3 \\ 1/3 \\ 0 \\ 1}w
                 \suchthat z,w\in\Re}
          \end{equation*}
        \partsitem The reduction
          \begin{align*}
            \begin{amat}[r]{4}
              1  &0  &1  &1  &4 \\
              2  &1  &0  &-1 &2 \\
              3  &1  &1  &0  &7
            \end{amat}
            &\grstep[-3\rho_1+\rho_3]{-2\rho_1+\rho_2}
            \begin{amat}[r]{4}
              1  &0  &1  &1  &4 \\
              0  &1  &-2 &-3 &-6\\
              0  &1  &-2 &-3 &-5
            \end{amat}                                       \\
            &\grstep{-\rho_2+\rho_3}
            \begin{amat}[r]{4}
              1  &0  &1  &1  &4 \\
              0  &1  &-2 &-3 &-6\\
              0  &0  &0  &0  &1
            \end{amat}
          \end{align*}
          shows that there is no solution\Dash the solution set is empty.
      \end{exparts}  
     \end{answer}
  \recommended \item \label{exer:SlvMatNot}
    Solve each system using matrix notation.
    Give each solution set in vector notation.
    \begin{exparts*}
      \partsitem \( \begin{linsys}[t]{3}
                  2x  &+  &y  &-  &z  &=  &1  \\
                  4x  &-  &y  &   &   &=  &3  
                \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{4}
                   x  &   &   &-  &z  &   &   &=  &1  \\
                      &   &y  &+  &2z &-  &w  &=  &3  \\
                   x  &+  &2y &+  &3z &-  &w  &=  &7  
               \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{4}
                   x  &-  &y  &+  &z  &   &   &=  &0  \\
                      &   &y  &   &   &+  &w  &=  &0  \\
                  3x  &-  &2y &+  &3z &+  &w  &=  &0  \\
                      &   &-y &   &   &-  &w  &=  &0  
               \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{5}
                   a  &+  &2b &+  &3c &+  &d  &-  &e  &=  &1  \\
                  3a  &-  &b  &+  &c  &+  &d  &+  &e  &=  &3  
               \end{linsys}  \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
      \partsitem The reduction
        \begin{equation*}
          \begin{amat}[r]{3}
            2  &1  &-1  &1  \\
            4  &-1 &0   &3
          \end{amat}
          \grstep{-2\rho_1+\rho_2}
          \begin{amat}[r]{3}
            2  &1  &-1  &1  \\
            0  &-3 &2   &1
          \end{amat}
        \end{equation*}
        ends with \( x \) and \( y \) leading, and with \( z \) free.
        Solving for \( y \) gives \( y=(1-2z)/(-3) \), and then substitution
        \( 2x+(1-2z)/(-3)-z=1 \) shows that \( x=((4/3)+(1/3)z)/2 \).
        Hence the solution set is this.
        \begin{equation*}
          \set{\colvec[r]{2/3 \\ -1/3 \\ 0}
               +\colvec[r]{1/6 \\ 2/3 \\ 1}z
              \suchthat z\in\Re}
        \end{equation*}
      \partsitem This application of Gauss's Method
        \begin{align*}
          \begin{amat}[r]{4}
            1  &0  &-1  &0  &1 \\
            0  &1  &2   &-1 &3 \\
            1  &2  &3   &-1 &7
          \end{amat}
          &\grstep{-\rho_1+\rho_3}
          \begin{amat}[r]{4}
            1  &0  &-1  &0  &1 \\
            0  &1  &2   &-1 &3 \\
            0  &2  &4   &-1 &6
          \end{amat}                                   \\
          &\grstep{-2\rho_2+\rho_3}
          \begin{amat}[r]{4}
            1  &0  &-1  &0  &1 \\
            0  &1  &2   &-1 &3 \\
            0  &0  &0   &1  &0
          \end{amat}
        \end{align*}
        leaves  \( x \), \( y \), and \( w \)  leading.
        The solution set is here.
        \begin{equation*}
          \set{\colvec[r]{1 \\ 3 \\ 0 \\ 0}
               +\colvec[r]{1 \\ -2 \\ 1 \\ 0}z
              \suchthat z\in\Re}
        \end{equation*}
      \partsitem This row reduction
        \begin{align*}
          \begin{amat}[r]{4}
            1  &-1 &1   &0  &0 \\
            0  &1  &0   &1  &0 \\
            3  &-2 &3   &1  &0 \\
            0  &-1 &0   &-1 &0
          \end{amat}
          &\grstep{-3\rho_1+\rho_3}
          \begin{amat}[r]{4}
            1  &-1 &1   &0  &0 \\
            0  &1  &0   &1  &0 \\
            0  &1  &0   &1  &0 \\
            0  &-1 &0   &-1 &0
          \end{amat}                                       \\
          &\grstep[\rho_2+\rho_4]{-\rho_2+\rho_3}
          \begin{amat}[r]{4}
            1  &-1 &1   &0  &0 \\
            0  &1  &0   &1  &0 \\
            0  &0  &0   &0  &0 \\
            0  &0  &0   &0  &0
          \end{amat}
        \end{align*}
        ends with \( z \) and \( w \) free.
        We have this solution set.
        \begin{equation*}
          \set{\colvec[r]{0 \\ 0 \\ 0 \\ 0}
               +\colvec[r]{-1 \\ 0 \\ 1 \\ 0}z
               +\colvec[r]{-1 \\ -1 \\ 0 \\ 1}w
              \suchthat z,w\in\Re}
        \end{equation*}
      \partsitem Gauss's Method done in this way
        \begin{equation*}
          \begin{amat}[r]{5}
            1  &2  &3   &1  &-1 &1  \\
            3  &-1 &1   &1  &1  &3
          \end{amat}
          \grstep{-3\rho_1+\rho_2}
          \begin{amat}[r]{5}
            1  &2  &3   &1  &-1 &1  \\
            0  &-7 &-8  &-2 &4  &0
          \end{amat}
        \end{equation*}
        ends with \( c \), \( d \), and \( e \) free.
        Solving for \( b \) shows that \( b=(8c+2d-4e)/(-7) \) and then 
        substitution
        \( a+2(8c+2d-4e)/(-7)+3c+1d-1e=1 \) shows that 
        \( a=1-(5/7)c-(3/7)d-(1/7)e \) and we have the solution set.
        \begin{equation*}
          \set{\colvec[r]{1 \\ 0 \\ 0 \\ 0 \\ 0}
               +\colvec[r]{-5/7 \\ -8/7 \\ 1 \\ 0 \\ 0}c
               +\colvec[r]{-3/7 \\ -2/7 \\ 0 \\ 1 \\ 0}d
               +\colvec[r]{-1/7 \\ 4/7 \\ 0 \\ 0 \\ 1}e
              \suchthat c,d,e\in\Re}
        \end{equation*}
    \end{exparts}  
   \end{answer}
  \recommended \item 
    The vector is in the set.
    What value of the parameters produces that vector?
    \begin{exparts}
      \partsitem $\colvec[r]{5 \\ -5}$,
        $\set{\colvec[r]{1 \\ -1}k\suchthat k\in\Re}$
      \partsitem $\colvec[r]{-1 \\ 2 \\ 1}$,
        $\set{\colvec[r]{-2 \\ 1 \\ 0}i
           +\colvec[r]{3 \\ 0 \\ 1}j\suchthat i,j\in\Re}$
      \partsitem $\colvec[r]{0 \\ -4 \\ 2}$,
        $\set{\colvec[r]{1 \\ 1 \\ 0}m
               +\colvec[r]{2 \\ 0 \\ 1}n\suchthat m,n\in\Re}$
    \end{exparts}
    \begin{answer}
      For each problem we get a system of linear equations by looking at the 
      equations of components.
      \begin{exparts}
       \partsitem $k=5$
       \partsitem The second components show that $i=2$, the third
       components show that $j=1$.
       \partsitem $m=-4$, $n=2$
      \end{exparts} 
    \end{answer}
  \item 
    Decide if the vector is in the set.
    \begin{exparts}
      \partsitem $\colvec[r]{3 \\ -1}$,
        $\set{\colvec[r]{-6 \\ 2}k\suchthat k\in\Re}$
      \partsitem $\colvec[r]{5 \\ 4}$,
        $\set{\colvec[r]{5 \\ -4}j\suchthat j\in\Re}$
      \partsitem $\colvec[r]{2 \\ 1 \\ -1}$,
        $\set{\colvec[r]{0 \\ 3 \\ -7}
             +\colvec[r]{1 \\ -1 \\ 3}r\suchthat r\in\Re}$
      \partsitem $\colvec[r]{1 \\ 0 \\ 1}$,
        $\set{\colvec[r]{2 \\ 0 \\ 1}j
            +\colvec[r]{-3 \\ -1 \\ 1}k\suchthat j,k\in\Re}$
    \end{exparts}
    \begin{answer}
      For each problem we get a system of linear equations by looking at the 
      equations of components.
      \begin{exparts}
        \partsitem Yes; take $k=-1/2$.
        \partsitem No; the system with equations $5=5\cdot j$ and
            $4=-4\cdot j$ has no solution.
        \partsitem Yes; take $r=2$.
        \partsitem No.
           The second components give $k=0$.
           Then the third components give $j=1$.
           But the first components don't check. 
      \end{exparts}
     \end{answer}
  \item \cite{Cleary}
    A farmer with 1200 acres is considering planting three different crops, 
    corn, soybeans, and oats.   
    The farmer wants to use all~$1200$ acres.  
    Seed corn costs \$$20$ per acre, while soybean and oat seed cost 
    \$$50$ and~\$$12$ per acre respectively.  
    The farmer has \$$40\,000$ available to buy seed and intends to 
    spend it all.
    \begin{exparts}  
      \item Use the information above to formulate two linear equations 
        with three unknowns and solve it.
     \item Solutions to the system are choices that the farmer can make.  
        Write down two reasonable solutions.
     \item Suppose that in the fall when the crops mature, the farmer 
        can bring in revenue of \$$100$ per acre for corn, 
        \$$300$ per acre for soybeans and \$$80$ per acre for oats.  
        Which of your two solutions in the prior part would have resulted 
        in a larger revenue? 
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \item Let $c$ be the number of acres of corn, $s$ be the number of 
          acres of soy, and $a$ be the number of acres of oats.
          \begin{equation*}
            \begin{linsys}{3}
              c   &+   &s   &+   &a   &=   &1200 \\ 
            20c   &+   &50s &+   &12a &=   &40\,000  
            \end{linsys}
            \grstep{-20\rho_1+\rho_2}
            \begin{linsys}{3}
              c   &+   &s   &+   &a   &=   &1200 \\ 
                  &    &30s &-   &8a  &=   &16\,000  
            \end{linsys}
          \end{equation*}
          To describe the solution set we can parametrize using $a$.
          \begin{equation*}
            \set{\colvec{c \\ s \\ a}
                 =\colvec{20\,000/30 \\ 16\,000/30 \\ 0}
                  +\colvec{-38/30 \\ 8/30 \\ 1}a
                 \suchthat a\in\Re}
          \end{equation*}
        \item There are many answers possible here. 
          For instance we can take $a=0$ to get $c=20\,000/30\approx 666.66$ and
          $s=16000/30\approx 533.33$.
          Another example is to take $a=20\,000/38\approx 526.32$, giving
          $c=0$ and $s=7360/38\approx 193.68$.
        \item Plug your answers from the prior part into 
          $100c+300s+80a$.
      \end{exparts}
    \end{answer}
  \item 
    Parametrize the solution set of this one-equation system.
    \begin{equation*}
      x_1+x_2+\cdots+x_n=0
    \end{equation*}
    \begin{answer}
      This system has one equation.
      The leading variable is \( x_1 \), the other variables are free.
      \begin{equation*}
        \set{\colvec{-1 \\ 1 \\ \vdotswithin{-1} \\ 0}x_2
             +\cdots+
             \colvec{-1 \\ 0 \\ \vdotswithin{-1} \\ 1}x_n
             \suchthat x_2,\ldots,x_n\in\Re}
      \end{equation*}  
     \end{answer}
  \recommended \item 
    \begin{exparts}
    \partsitem Apply Gauss's Method to the left-hand side to solve
      \begin{equation*}
        \begin{linsys}{4}
          x  &+  &2y  &    &    &-   &w   &=   &a   \\
         2x  &   &    &+   &z   &    &    &=   &b   \\
          x  &+  &y   &    &    &+   &2w  &=   &c   
        \end{linsys}
      \end{equation*}
      for \( x \), \( y \), \( z \), and \(  w \), in terms of the 
      constants $a$, $b$, and $c$.
    \partsitem Use your answer from the prior part to solve this.
      \begin{equation*}
        \begin{linsys}{4}
          x  &+  &2y  &    &    &-   &w   &=   &3   \\
         2x  &   &    &+   &z   &    &    &=   &1   \\
          x  &+  &y   &    &    &+   &2w  &=   &-2
        \end{linsys}
      \end{equation*}
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Gauss's Method here gives
          \begin{align*}
            \begin{amat}{4}
              1  &2  &0  &-1  &a  \\
              2  &0  &1  &0   &b  \\
              1  &1  &0  &2   &c
            \end{amat}
            &\grstep[-\rho_1+\rho_3]{-2\rho_1+\rho_2}
            \begin{amat}{4}
              1  &2  &0  &-1  &a  \\
              0  &-4 &1  &2   &-2a+b  \\
              0  &-1 &0  &3   &-a+c
            \end{amat}                                  \\
            &\grstep{-(1/4)\rho_2+\rho_3}
            \begin{amat}{4}
              1  &2  &0    &-1  &a  \\
              0  &-4 &1    &2   &-2a+b  \\
              0  &0  &-1/4 &5/2 &-(1/2)a-(1/4)b+c
            \end{amat}
          \end{align*}
          leaving \( w \) free.
          Solve: \(  z=2a+b-4c+10w \),
          and \( -4y=-2a+b-(2a+b-4c+10w)-2w \) so
          \( y=a-c+3w \), and
          \( x=a-2(a-c+3w)+w=-a+2c-5w. \)
          Therefore the solution set is this.
          \begin{equation*}
             \set{\colvec{-a+2c \\ a-c \\ 2a+b-4c \\ 0}
                  +\colvec{-5 \\ 3 \\ 10 \\ 1}w
                  \suchthat w\in\Re}
          \end{equation*}
        \partsitem Plug in with \( a=3 \), \( b=1 \), and \( c=-2 \).
          \begin{equation*}
             \set{\colvec[r]{-7 \\ 5 \\ 15 \\ 0}
                  +\colvec[r]{-5 \\ 3 \\ 10 \\ 1}w
                  \suchthat w\in\Re}
          \end{equation*}
      \end{exparts}  
     \end{answer}
  \recommended \item 
    Why is the comma needed in the notation `\( a_{i,j} \)'
    for matrix entries?
    \begin{answer}
       Leaving the comma out, say by writing \( a_{123} \),
       is ambiguous because it could mean $a_{1,23}$ or $a_{12,3}$.  
    \end{answer}
  \recommended \item 
    Give the \( \nbyn{4} \) matrix whose
    \( i,j \)-th entry is
    \begin{exparts*}
      \partsitem \( i+j \);
      \partsitem \( -1 \) to the \( i+j \) power.
    \end{exparts*}
    \begin{answer}
      \begin{exparts*}
        \partsitem \(
           \begin{mat}[r]
             2  &3  &4  &5  \\
             3  &4  &5  &6  \\
             4  &5  &6  &7  \\
             5  &6  &7  &8
           \end{mat} \)
        \partsitem \(
           \begin{mat}[r]
             1  &-1  &1   &-1  \\
            -1  &1   &-1  &1  \\
             1  &-1  &1   &-1  \\
            -1  &1   &-1  &1
           \end{mat} \)
      \end{exparts*}  
    \end{answer}
  \item  
    For any matrix \( A \), the 
    \definend{transpose}\index{transpose}
    \index{matrix!transpose}
    of \( A \), written
    \( \trans{A} \), is the matrix whose columns are the rows of \( A \).
    Find the transpose of each of these.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
                  1  &2  &3  \\
                  4  &5  &6
               \end{mat}  \)
      \partsitem \( \begin{mat}[r]
                  2  &-3 \\
                  1  &1
               \end{mat}  \)
      \partsitem \( \begin{mat}[r]
                  5  &10 \\
                 10  &5
               \end{mat}  \)
      \partsitem \( \colvec[r]{1 \\ 1 \\ 0} \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts*}
        \partsitem \( \begin{mat}[r]
                   1  &4  \\
                   2  &5  \\
                   3  &6
                 \end{mat}  \)
        \partsitem \( \begin{mat}[r]
                   2  &1  \\
                  -3  &1
                 \end{mat}  \)
        \partsitem \( \begin{mat}[r]
                   5  &10 \\
                  10  &5
                 \end{mat}  \)
        \partsitem \( \rowvec{1 &1 &0}  \)
      \end{exparts*}  
     \end{answer}
  \recommended \item 
    \begin{exparts}
      \partsitem Describe all functions \( f(x)=ax^2+bx+c \) 
        such that \( f(1)=2 \) and \( f(-1)=6 \).
      \partsitem Describe all functions \( f(x)=ax^2+bx+c \) 
        such that \( f(1)=2 \).
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Plugging in \( x=1 \) and \( x=-1 \) gives
          \begin{equation*}
            \begin{linsys}{3}
              a  &+  &b   &+  &c  &=  &2  \\
              a  &-  &b   &+  &c  &=  &6  
            \end{linsys}
            \grstep{-\rho_1+\rho_2}
            \begin{linsys}{3}
              a  &+  &b   &+  &c  &=  &2  \\
                 &   &-2b &   &   &=  &4  
              \end{linsys}
          \end{equation*}
          so the set of functions is
          \( \set{f(x)=(4-c)x^2-2x+c\suchthat c\in\Re} \).
        \partsitem Putting in \( x=1 \) gives
          \begin{equation*}
            \begin{linsys}{3}
              a  &+  &b   &+  &c  &=  &2  
            \end{linsys}
          \end{equation*}
          so the set of functions is
          \( \set{f(x)=(2-b-c)x^2+bx+c\suchthat b,c\in\Re} \).
      \end{exparts}  
    \end{answer}
  \item Show that any set of five points from the plane \( \Re^2 \) lie on a
    common conic section, that is, they all satisfy some equation of the
    form \( ax^2+by^2+cxy+dx+ey+f=0 \) where some of \( a,\,\ldots\,,f \)
    are nonzero.
    \begin{answer}
      On plugging in the five pairs $(x,y)$ we get a system with the
      five equations and six unknowns $a$, \ldots, $f$.
      Because there are more unknowns than equations, if no inconsistency
      exists among the equations then there are infinitely many solutions
      (at least one variable will end up free).

      But no inconsistency can exist because $a=0$, \ldots, $f=0$ is a 
      solution (we are only using this zero solution to show that the system
      is consistent\Dash the prior paragraph shows that
      there are nonzero solutions). 
    \end{answer}
  \item 
    Make up a four equations/four unknowns system having
    \begin{exparts}
      \partsitem a one-parameter solution set;
      \partsitem a two-parameter solution set;
      \partsitem a three-parameter solution set.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
      \partsitem Here is one\Dash the fourth equation is redundant 
        but still OK.
        \begin{equation*}
          \begin{linsys}{4}
             x  &+  &y  &-  &z  &+  &w  &=  &0  \\
                &   &y  &-  &z  &   &   &=  &0  \\
                &   &   &   &2z &+  &2w &=  &0  \\
                &   &   &   &z  &+  &w  &=  &0
          \end{linsys}
        \end{equation*}
      \partsitem Here is one.
        \begin{equation*}
          \begin{linsys}{4}
             x  &+  &y  &-  &z  &+  &w  &=  &0  \\
                &   &   &   &   &   &w  &=  &0  \\
                &   &   &   &   &   &w  &=  &0  \\
                &   &   &   &   &   &w  &=  &0
          \end{linsys}
        \end{equation*}
      \partsitem This is one.
        \begin{equation*}
          \begin{linsys}{4}
             x  &+  &y  &-  &z  &+  &w  &=  &0  \\
             x  &+  &y  &-  &z  &+  &w  &=  &0  \\
             x  &+  &y  &-  &z  &+  &w  &=  &0  \\
             x  &+  &y  &-  &z  &+  &w  &=  &0 
          \end{linsys}
        \end{equation*}
    \end{exparts}  
   \end{answer}
  \puzzle \item 
    \cite{Shepelev}
    This  puzzle  is  from  a  Russian   web-site
    \texttt{http://www.arbuz.uz/}  and  there are many solutions
    to it, but mine uses  linear  algebra  and  is  very
    naive.   There's   a   planet  inhabited  by  arbuzoids
   (watermeloners, to  translate  from  Russian). 
   Those creatures are found in three colors: red, green and blue.  
   There  are  $13$~red
   arbuzoids,  $15$~blue  ones, and $17$~green. 
   When
   two differently colored arbuzoids meet,  they
   both change to the third color.

   The question is, can it ever happen that all
   of them assume the same color?
    \begin{answer}
       \answerasgiven
       My solution was to define the numbers  of  arbuzoids
       as $3$-dimensional vectors, and express all possible
       elementary transitions as such vectors, too:
       \begin{center}
         \begin{tabular}{rr}
           R: &$13$  \\
           G: &$15$  \\
           B: &$17$
         \end{tabular}
         \qquad
         Operations:
         $\colvec[r]{-1 \\ -1 \\ 2}$, 
         $\colvec[r]{-1 \\ 2 \\ -1}$, 
         and 
         $\colvec[r]{2 \\ -1 \\ -1}$
       \end{center}
       Now, it is enough to check whether the  solution  to
       one  of  the  following  systems of linear equations
       exists:
       \begin{equation*}
         \colvec[r]{13 \\ 15 \\ 17}
         +x\colvec[r]{-1 \\ -1  \\ 2}
         +y\colvec[r]{-1 \\ 2 \\ -1}
         +\colvec[r]{2 \\ -1 \\ -1}
         =\colvec[r]{0 \\ 0 \\ 45}
         \qquad
         \text{(or $\colvec[r]{0 \\ 45 \\ 0}$ or $\colvec[r]{45 \\ 0 \\ 0}$)}
       \end{equation*}
       Solving
       \begin{equation*}
         \begin{amat}[r]{3}
          -1  &-1 &2  &-13  \\
          -1  &2  &-1 &-15  \\
           2  &-1 &-1 &28
         \end{amat}
         \grstep[2\rho_1+\rho_3]{-\rho_1+\rho_2}
         \repeatedgrstep{\rho_2+\rho_3}
         \begin{amat}[r]{3}
          -1  &-1 &2  &-13  \\
           0  &3  &-3 &-2  \\
           0  &0  &0  &0
         \end{amat}
       \end{equation*}
       gives $y+2/3=z$ so if the number of transformations $z$ is an integer
       then $y$ is not.
       The other two systems give similar conclusions so there is no
       solution.
    \end{answer}
  \puzzle \item 
    \cite{USSROlympiad174}
    \begin{exparts}
      \partsitem Solve the system of equations.
        \begin{equation*}
          \begin{linsys}{2}
            ax  &+  &y  &=  &a^2  \\
             x  &+  &ay &=  &1
         \end{linsys}
        \end{equation*}
        For what values of $a$ does the system fail to have solutions, and
        for what values of $a$ are there infinitely many solutions?
      \partsitem Answer the above question for the system.
        \begin{equation*}
          \begin{linsys}{2}
            ax  &+  &y  &=  &a^3  \\    
             x  &+  &ay &=  &1
          \end{linsys}
        \end{equation*}
    \end{exparts}
    \begin{answer}
       \answerasgiven
       \begin{exparts}
        \partsitem Formal solution of the system yields
          \begin{equation*}
            x=\frac{a^3-1}{a^2-1}  
            \qquad
            y=\frac{-a^2+a}{a^2-1}.
          \end{equation*}
          If $a+1\neq 0$ and $a-1\neq 0$, then the system has the single
          solution
          \begin{equation*}
            x=\frac{a^2+a+1}{a+1}
            \qquad
            y=\frac{-a}{a+1}.
          \end{equation*}
          If $a=-1$, or if $a=+1$, then the formulas are meaningless; in the
          first instance we arrive at the system
          \begin{equation*}
            \left\{ 
            \begin{linsys}{2}
              -x &+  &y  &=  &1 \\
               x &-  &y  &=  &1
            \end{linsys}\right.
          \end{equation*}
          which is a contradictory system.
          In the second instance we have
          \begin{equation*}
            \left\{
            \begin{linsys}{2}
               x &+  &y  &=  &1 \\
               x &+  &y  &=  &1
            \end{linsys}\right.
          \end{equation*}
          which has an infinite number of solutions (for example, for 
          $x$ arbitrary, $y=1-x$).
        \partsitem Solution of the system yields
          \begin{equation*}
            x=\frac{a^4-1}{a^2-1}
            \qquad
            y=\frac{-a^3+a}{a^2-1}.
          \end{equation*}
          Here, is $a^2-1\neq 0$, the system has the single solution
          $x=a^2+1$, $y=-a$.
          For $a=-1$ and $a=1$, we obtain the systems
          \begin{equation*}
            \left\{
            \begin{linsys}{2}
              -x &+  &y  &=  &-1 \\
               x &-  &y  &=  &1
            \end{linsys}\right.
            \qquad
            \left\{
            \begin{linsys}{2}
               x &+  &y  &=  &1 \\
               x &+  &y  &=  &1
            \end{linsys}\right.
         \end{equation*}
         both of which have an infinite number of solutions.
      \end{exparts}
    \end{answer}
  \puzzle \item 
    \cite{MathMag52p48}
    In air a gold-sur\-faced sphere weighs \( 7588 \)
    grams.
    It is known that it may contain one or more of the metals aluminum,
    copper, silver, or lead.
    When weighed successively under standard conditions in water, benzene,
    alcohol, and glycerin its respective weights are \( 6588 \), \( 6688 \),
    \( 6778 \), and \( 6328 \) grams.
    How much, if any, of the forenamed metals does it contain if the
    specific gravities of the designated substances are taken to be as follows?
    \begin{center}
       \begin{tabular}{lrclr}
         Aluminum  &\( 2.7 \)
            &\makebox[3em]{\mbox{}\hfill\mbox{}} &Alcohol &0.81 \\
         Copper    &\( 8.9 \)  &     &Benzene   &\( 0.90 \) \\
         Gold      &\( 19.3 \) &     &Glycerin &\( 1.26 \) \\
         Lead      &\( 11.3 \) &     &Water     &\( 1.00 \) \\
         Silver    &\( 10.8 \)
       \end{tabular}
    \end{center}
    \begin{answer}
      \answerasgiven
      Let \( u \), \( v \), \( x \), \( y \), \( z \) be the volumes in
      \( {\rm cm}^3 \) of Al, Cu, Pb, Ag, and Au, respectively, contained in
      the sphere, which we assume to be not hollow.
      Since the loss of weight in water (specific gravity \( 1.00 \)) is
      \( 1000 \) grams, the volume of the sphere is \( 1000\mbox{ cm}^3 \).
      Then the data, some of which is superfluous, though consistent, leads to
      only \( 2 \) independent equations, one relating volumes and the
      other, weights.
      \begin{equation*}
        \begin{linsys}{5}
           u  &+  &v    &+  &x     &+  &y     &+  &z     &=  &1000  \\
        2.7u  &+  &8.9v &+  &11.3x &+  &10.5y &+  &19.3z &=  &7558
        \end{linsys}
      \end{equation*}
      Clearly the sphere must contain some aluminum to bring its mean specific
      gravity below the specific gravities of all the other metals.
      There is no unique result to this part of the problem, for the amounts
      of three metals may be chosen arbitrarily, provided that the choices
      will not result in negative amounts of any metal.

      If the ball contains only aluminum and gold, there are
      \( 294.5\mbox{ cm}^3 \) of gold and \( 705.5\mbox{ cm}^3 \) of aluminum.
      Another possibility is \( 124.7\mbox{ cm}^3 \) each of Cu, Au, Pb, and
      Ag and \( 501.2\mbox{ cm}^3  \) of Al.   
    \end{answer}
\end{exercises}



  
\item 
    Prove that each operation of Gauss's Method is reversible.
    That is, show that if two systems are related by a row operation
    $S_1\rightarrow S_2$ then there is a row operation to go back
    $S_2\rightarrow S_1$.
    \begin{answer}
      Reverse a row swap $\rho_i\leftrightarrow\rho_j$ by swapping 
      back $\rho_j\leftrightarrow\rho_i$.
      Reverse the $k\rho_i$ step of multiplying  \( k\neq 0  \) 
      on both sides of a row
      by  dividing through~$(1/k)\rho_i$.

      The row combination case is the nontrivial one.
      The operation $k\rho_i+\rho_j$
      results in this $j$-th row.
      \begin{equation*}
        k\cdot a_{i,1}+a_{j,1}+\cdots+k\cdot a_{i,n}+a_{j,n}=k\cdot d_i+d_j
      \end{equation*}
      The $i$-th row unchanged because of the $i\neq j$ restriction.
      Because the $i$-th row is unchanged, the operation $-k\rho_i+\rho_j$ 
      returns the $j$-th row to its original state.

      (Observe that the \( i=j \) conditino on the $k\rho_i+\rho_j$ 
       is needed, or else this could happen
       \begin{equation*}
         \begin{linsys}{2}
           3x  &+  &2y  &=  &7  
         \end{linsys}
         \grstep{2\rho_1+\rho_1}
         \begin{linsys}{2}
           9x  &+  &6y  &=  &21 
          \end{linsys}                 
         \grstep{-2\rho_1+\rho_1}
         \begin{linsys}{2}
          -9x  &-  &6y  &=  &-21 
         \end{linsys}
       \end{equation*}
       and so the result wouldn't hold.)
    \end{answer}
 

S06 (Fall 2015)




Given
\[
A=
\begin{bmatrix}
1 & 2 & 3\\
2 & 3 & 4\\
3 & 5 & 6
\end{bmatrix},\;
B=
\begin{bmatrix}
2 & 1  & -4\\
-1 & 2 & 2\\
-3 & 4 & 6
\end{bmatrix}
\]
\begin{enumerate}
\item[a.] \textit{(4 marks)} Find the $\text{adj}(A)$.
\item[b.] \textit{(4 marks)} Evaluate $\text{det}(B^{101}\text{adj}(A)+BA^{2015})$.

Given
\[
A=
\begin{bmatrix}
1 & 1 & 1 & 1 & 9 & 3 & 4 & 1 & 0 & 9\\
0 & 2 & 1 & 1 & 4 & 9 & 2 & 7 & 7 & 9\\
0 & 0 & 1 & 1 & 1 & 4 & 9 & 2 & 7 & 7\\
0 & 0 & 0 & 2 & 1 & 1 & 4 & 9 & 2 & 7\\
0 & 0 & 0 & 0 & 1 & 1 & 1 & 4 & 9 & 2\\
0 & 0 & 0 & 0 & 0 & 2 & 1 & 1 & 4 & 9\\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 4\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 1 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 2\\
\end{bmatrix},\;\;
B=
\begin{bmatrix}
1 & 1 & 2 & 6\\
2 & 3 & 3 & -3\\
-3 & 3 & 1 & 9\\
0 & 1 & -1 & -12
\end{bmatrix}
\]
\begin{enumerate}
\item[a.] \textit{(4 marks)} Evaluate $\operatorname{det}(B)$.
\item[b.] \textit{(4 marks)} If $C$ is a square matrix and $\operatorname{det}(\frac{\text{det}(B)}{2}C^TA^3)=\pi$ then find $\text{det}(C)$, if possible.
\end{enumerate}


Given the following system
\begin{eqnarray*}
x+z=1\\
y+z=1
\end{eqnarray*}
\begin{enumerate}
\item[a.] \textit{(2 marks)} Express a general solution of this system as a particular solution of the system plus a general solution of the associated homogeneous system.
\item[b.] \textit{(2 marks)} Give a geometric interpretation of the result of part a.  Discuss the general solution geometrically with respect to the associated homogeneous system.
\end{enumerate}



Determine whether the set $\{ 1, e^x, \arctan x\}$ is linearly independent on $\mathbb{R}^+$

S07 (Fall 2015)


Let 
\[
A =
\begin{bmatrix}
1 & -1 & 3 & 2\\
0 & 1 & 4 & 1\\
1 & 0 & 8 & 6
\end{bmatrix},\;
B = 
\begin{bmatrix}
1 & -1 & 4 & 5\\
-2 & 1 & -11 & -8\\
-1 & 2 & 2 & 2
\end{bmatrix}
\]
\begin{enumerate}
\item[a.] \textit{(5 marks)} Show that $A$ and $B$ have the same reduced row echelon matrix.
\item[b.] \textit{(3 marks)} Justify using a. that $A$ and $B$ are row equivalent. 
\end{enumerate}

Given
\[
B=
\begin{bmatrix}
1 & 1 & 1 & 1 & 9 & 3 & 4 & 1 & 0 & 9\\
0 & 2 & 1 & 1 & 4 & 9 & 2 & 7 & 7 & 9\\
0 & 0 & 3 & 1 & 1 & 4 & 9 & 2 & 7 & 7\\
0 & 0 & 0 & 4 & 1 & 1 & 4 & 9 & 2 & 7\\
0 & 0 & 0 & 0 & 5 & 1 & 1 & 4 & 9 & 2\\
0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 4 & 9\\
0 & 0 & 0 & 0 & 0 & 0 & 2 & 1 & 1 & 4\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 3 & 1 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 4 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 5\\
\end{bmatrix}
\]
\begin{enumerate}
\item[b.] \textit{(4 marks)} If $C$ is a $10\times 10$ invertible matrix then evaluate $\text{det}(C^{101}\text{adj}(B)(C^{-1})^{101})$.

Given
\[
A=
\begin{bmatrix}
4 & 3 & -2 & 8 \\
2 & 3 & 1 & -4 \\
-1 & 2 & -1 & 4 \\
1 & 2 & 2 & -8 \\
\end{bmatrix},\;\;
B=
\begin{bmatrix}
1 & 0 & 2 & 6\\
2 & 3 & 0 & 1\\
-3 & 1 & 1 & 0\\
1 & 1 & -1 & 0
\end{bmatrix}
\]
\begin{enumerate}
\item[a.] \textit{(5 marks)} Evaluate $\operatorname{det}(A)$ and $\operatorname{det}(B)$.
\item[b.] \textit{(2 marks)} Prove or disprove: There exists $C$ a square $4\times 4$ invertible matrix such that $ABC=I$.
\end{enumerate}





Winter 2015

Consider
\[
A=
\begin{bmatrix}
1 & 2 & 0\\
2 & 0 & 7\\
3 & 2 & 0
\end{bmatrix}.
\]
\begin{enumerate}
\item[a.] \textit{(5 marks)} Find $A^{-1}$.
\item[b.] \textit{(2 marks)} Determine $\operatorname{tr}(A^{-1})$ and $\operatorname{tr}(A^{-1}A)$.

The augmented matrix of a linear system is given by
\[
\begin{bmatrix}
1 & 2 & 3 & 4 & \pi\\
0 & \sqrt{2} & 4 & 5 & 6\\
0 & 0 & 0 & a & b\\
\end{bmatrix}
\]
If possible for what values of $a$ and $b$ there is
\begin{enumerate}
\item[a.] \textit{(2 marks)} no solution? Justify.
\item[b.] \textit{(2 marks)} exactly one solution? Justify.
\item[c.] \textit{(2 marks)} infinitely many solutions? Justify.

Given
\[
A=
\begin{bmatrix}
10 & 1 & 1 & 1 & 9 & 3 & 4 & 1 & 0 & 9\\
0 & 9 & 1 & 1 & 4 & 9 & 2 & 7 & 7 & 9\\
0 & 0 & 8 & 1 & 1 & 4 & 9 & 2 & 7 & 7\\
0 & 0 & 0 & 7 & 1 & 1 & 4 & 9 & 2 & 7\\
0 & 0 & 0 & 0 & 6 & 1 & 1 & 4 & 9 & 2\\
0 & 0 & 0 & 0 & 0 & 5 & 1 & 1 & 4 & 9\\
0 & 0 & 0 & 0 & 0 & 0 & 4 & 1 & 1 & 4\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 3 & 1 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{bmatrix},\;\;
B=
\begin{bmatrix}
1 & 1 & 2 & 6\\
2 & 3 & 3 & -3\\
-3 & 3 & 1 & 9\\
0 & 1 & -1 & -12
\end{bmatrix},\;\;
C=
\begin{bmatrix}
-1 & 3 & 2 & 3\\
2 & 0 & 3 & 0\\
-4 & 0 & 1 & 2\\
1 & 5 & 0 & -3
\end{bmatrix}
\]
\begin{enumerate}
\item[a.] \textit{(4 marks)} Evaluate $\operatorname{det}(2\operatorname{adj}(A)+3A^{-1})$, if possible.
\item[b.] \textit{(4 marks)} If $D$ is a $4\times 4$ matrix and $B^{-1}D^2=I$ then determine $\operatorname{det}(D)$, if possible.
\item[c.] \textit{(4 marks)} Compute $\operatorname{det}(C)$.

Determine whether the following is a vector space:
\[
\mathcal{Y}=\left\{(1,y)\;|\;y\in\mathbb{R}\right\}
\]
with the following addition and scalar multiplication.
\[
(1, y_1)+(1, y_2) = (1, y_1^2+y_2^2)
\]
and
\[
k(1,y)=(1,ky)
\]
If it is not a vector space, clearly state and show which axiom fails.


Find all values of $z$ such that the triangle with vertices $A(-1, 2, -3)$, $B(4,-5,6)$ and $C(1, -2, z)$ has area equal to $31$.

In a vector space, the span of any subset is a subspace. In addition, the span of a subset of a vector space is the smallest subspace containing all vectors of the subset.


Given
\[
A=
\begin{bmatrix}
9 & 9 & 1 & 1 & 9 & 3 & 4 & 1 & 0 & 9\\
9 & 9 & 1 & 1 & 4 & 9 & 2 & 7 & 7 & 9\\
0 & 0 & 7 & 1 & 1 & 4 & 9 & 2 & 7 & 7\\
0 & 0 & 0 & 7 & 1 & 1 & 4 & 9 & 2 & 7\\
0 & 0 & 0 & 0 & 2 & 1 & 1 & 4 & 9 & 2\\
0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 4 & 9\\
0 & 0 & 0 & 0 & 0 & 0 & 4 & 1 & 1 & 4\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 2\\
\end{bmatrix},\;\;
B=
\begin{bmatrix}
-2 & 3 & 2 & 6\\
0 & 2 & 3 & -3\\
0 & 0 & 1 & 9\\
0 & 0 & 0 & -12
\end{bmatrix},\;\;
C=
\begin{bmatrix}
a & b\\
c & d 
\end{bmatrix},\;\;
D=
\begin{bmatrix}
b & d \\
3a-2b & 3c-2d 
\end{bmatrix}
\]

\begin{enumerate}
\item[a.] \textit{(4 marks)} If $F$ is a $10\times 10$ matrix show that $AF$ is not invertible.
\item[b.] \textit{(4 marks)} If $E$ is an invertible matrix then evaluate $\operatorname{det}(E^{-1})^4\operatorname{det}(\operatorname{det}(E)\operatorname{adj}(B))$, justify fully.
\item[c.] \textit{(4 marks)} If $\operatorname{det}(D)=2$ then determine $\operatorname{det}(C)$.
\end{enumerate}
\newpage
\textbf{Question 3.} If $A$ is an $n\times n$ matrix which entries are all divisible by 2.
\begin{enumerate}
\item[a.] \textit{(2 marks)} Justify that the entries of $\operatorname{adj}(A)$ are all divisible by 2.
\item[b.] \textit{(2 marks)} Prove using a. and $\det(A)=2$ that the entries of $A^{-1}$ are all integers.
\end{enumerate} 


Let $\mathcal{H}=\{A |\; \text{$A$ is a $2\times 2$ matrix and $A^T=-A$} \}$
with the usual addition and scalar multiplication.
\begin{enumerate}
\item[a.] \textit{(2 marks)} Give an example of a non-zero matrix in $\mathcal{H}$. Justify.
\item[b.] \textit{(2 marks)} Does $\mathcal{H}$ satisfy closure under vector addition? Justify.
\item[c.] \textit{(2 marks)} Does $\mathcal{H}$ contain the zero vector of $\mathcal{M}_{2\times 2}$ (the vector space of $2\times2$ matrices)? Justify.
\item[d.] \textit{(2 marks)} Does $\mathcal{H}$ satisfy closure under scalar multiplication? Justify.
\item[e.] \textit{(2 marks)} Is $\mathcal{H}$ a vector subspace of $\mathcal{M}_{2\times 2}$ (the vector space of $2\times2$ matrices)? Justify.

S05 Fall2014




Show that $||\vec{u}+\vec{v}||^2+||\vec{u}-\vec{v}||^2 = 2(||\vec{u}||^2+||\vec{v}||^2)$. \textit{Bonus (1 mark): what does that say about parallelograms?}





Given
\[
A(1,2),\;B(-1,1),\;C(3,1)  
\]
\begin{enumerate}
\item[a.] \textit{(3 marks)} Find the area of the parallelogram generated by $\vec{AB}$ and $\vec{AC}$.
\item[b.] \textit{(1 mark)} Find the area of the triangle with vertices $A$, $B$ and $C$.
\item[c.] \textit{(2 marks)} Find the length of the altitude of the triangle $ABC$ from vertex $C$ to side $AB$.
\item[d.] \textit{(1 mark)} Find the shortest distance from $C$ to the line that contains $A$ and $B$.
\end{enumerate}

Let $\mathcal{H}=\left\{A=\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}
\Bigg|\; A\begin{bmatrix}
1\\
1
\end{bmatrix}
=
\begin{bmatrix}
0\\
0
\end{bmatrix} \right\}$
with the usual addition and scalar multiplicatio
\begin{enumerate}
\item[a.] \textit{(2 marks)} Give an example of a non-zero matrix in $\mathcal{H}$? Justify.
\item[b.] \textit{(2 marks)} Does $\mathcal{H}$ satisfy closure under vector addition? Justify.
\item[c.] \textit{(2 marks)} Does $\mathcal{H}$ contain the zero vector of $\mathcal{M}_{2\times 2}$ (the vector space of $2\times2$ matrices)? Justify.
\item[d.] \textit{(2 marks)} Does $\mathcal{H}$ satisfy closure under scalar multiplication? Justify.
\item[e.] \textit{(2 marks)} Is $\mathcal{H}$ a vector subspace of $\mathcal{M}_{2\times 2}$ (the vector space of $2\times2$ matrices)? Justify.
\end{enumerate}

Prove or disprove: For all vector vectors $\vec{u}$, $\vec{v}$ and $\vec{w}$ in 3-space, the vectors $(\vec{u}\times\vec{v})\times\vec{w}$ and $\vec{u}\times(\vec{v}\times\vec{w})$ are the same.



matrix algebra:

\it Let $A$ be an $n \times n$ matrix. Let $m \in \N$. As you would expect, we define $A^m$ to be the product of $A$ with itself $m$ times. Notice that this makes 
	sense as matrix multiplication is associative. 
	\be
	\it Compute $A^4$ for $A = \begin{bmatrix} 1 & -2 \\ 1 & -1 \end{bmatrix}$. 
	\it Provide a counter-example to the statement: For any $2 \times 2$ matrices $A$ and $B$, $(AB)^2 = A^2 B^2$.
	\ee

\it Prove that for all $m \times n$ matrices $A$, $B$, and $C$, if $A + C = B + C$, then $A = B$.

\it Let $\Theta$ be the $m \times n$ zero matrix. Prove that for any $m \times n$ matrix $A$, $-A = (-1)A$ and $0A = \Theta$.

\it Let $\Theta$ be the $m \times n$ zero matrix. Prove that for any $r \in \R$ and $m \times n$ matrix $A$, if $rA = \Theta$, then $r = 0$ or $A = \Theta$.

\it We have seen an example of two $2 \times 2$ matrices $A$ and $B$ for which $AB \not= BA$, showing us that matrix multiplication is not commutative.
	However, it's more that just not commutative, it's soooooo not commutative. Doing the following problems will illustrate what we mean by this.
	\be
	\it Find two matrices $A$ and $B$ for which $AB$ and $BA$ are defined, but have different sizes.
	\it Find two matrices $A$ and $B$ for which $AB$ is defined, but $BA$ is not.
	\ee
\it Let $\Theta$ be the $2 \times 2$ zero matrix and $I$ the $2 \times 2$ identity matrix. Provide a counter-example to each of the following statements:
	\be
	\it For any $2 \times 2$ matrices $A$ and $B$, if $AB = \Theta$, then $A = \Theta$ or $B = \Theta$.
	\it For any $2 \times 2$ matrices, $A$, $B$, and $C$, if $AB = AC$, then $B = C$.
	\it For any $2 \times 2$ matrix $A$, if $A^2 = A$, then $A = \Theta$ or $A = I$.
	\ee

\it Suppose that we have a homogeneous system of linear equations whose matrix equation is given by $A\bar{x} = \bar{\th}$ where $A$ is the $m \times n$ matrix of 
	coefficients, $\bar{x}$ is the column matrix of the $n$ variables, and $\bar{\th}$ is the column matrix of the $m$ zeroes. Use the properties of matrix
	arithmetic to show that for any solutions $\bar{u}$ and $\bar{v}$ to the system and $r \in \R$, $\bar{u+v}$ and $r\bar{u}$ are also solutions.

\it Consider the system of linear equations:  
\begin{align*}
x_1 + x_2 + x_3 + x_4 &= 3 \\
x_1 - x_2 + x_3 + x_4 &= 5 \\
      x_2 - x_3 - x_4 &= -4 \\
x_1 + x_2 - x_3 - x_4 &= -3.
\end{align*}
\be
\it Express the system as a matrix equation.
\it Use matrix multiplication to determine whether or not $\bar{u} = \begin{bmatrix} 1 \\ -1 \\ 1 \\ 2 \end{bmatrix}$ and $\bar{v} = \begin{bmatrix} -1 \\ 2 \\ 0 \\ 3 \end{bmatrix}$ are solutions to the system.
\ee


\it Provide a counter-example to the statement: For any $n \times n$ invertible matrices $A$ and $B$, $A + B$ is invertible.

\it Find an example of a $2 \times 2$ nonidentity matrix whose inverse is its transpose.

\it Using the matrices in Problem \#1, compute each of the following, if possible.
	\bttt
	(a) $3A - 2E^T$ \> (b) $A^T B$ \> (c) $D^T (F + G^T)$ \\
	(d) $3A^T - 2E$ \> (e) $C C^T + FG$ \> (f) $(F^T + G)D$
	\etb

\it Let $A$ be an $n \times n$ invertible matrix.
	\be
	\it Prove that for all $m \in \N$, $A^m$ is invertible and determine a formula for its inverse in terms of $A^{-1}$. Hint: Use induction and the fact that
		$A^{n+1} = A^n A$.
	\it Prove that $A^T$ is invertible and determine its inverse in terms of $A^{-1}$. Hint: Use Proposition 3.
	\ee

\it We say that an $n \times n$ matrix $A$ is \ul{symmetric} provided $A^T = A$; we say that $A$ is \ul{skew-symmetric} provided $A^T = -A$. 
	\be
	\it Give examples of symmetric and skew-symmetric $2 \times 2$, $3 \times 3$, and $4 \times 4$ matrices.
	\it What can you say about the main diagonal of a skew-symmetric matrix?
	\it Prove that for any $n \times n$ matrix $A$, $A + A^T$, $AA^T$, and $A^T A$ are symmetric and $A - A^T$ is skew-symmetric.
	\it Prove that any $n \times n$ can be written as the sum of a symmetric and skew-symmetric matrices. Hint: Did you do part (c) yet?
	\ee

Application JH


  \item 
    In the first network that we analyzed, with the three resistors  
    in series, we just added to get
    that they acted together like a single resistor of $10$~ohms.
    We can do a similar thing for parallel circuits. 
    In the second circuit analyzed,
    \begin{center}
      \includegraphics{ch1.36}
    \end{center}
    the electric current through the battery is $25/6$~amperes.
    Thus, the parallel portion is 
    \definend{equivalent}\index{resistance:equivalent}
    to a single resistor of 
    $20/(25/6)=4.8$~ohms. 
    \begin{exparts}
      \partsitem What is the equivalent resistance if we change
         the $12$~ohm resistor to $5$~ohms?
      \partsitem What is the equivalent resistance if the two are each
         $8$~ohms?
      \partsitem Find the formula for the equivalent resistance if
         the two resistors in parallel are $r_1$~ohms and $r_2$~ohms.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem 
          Using the variables from the earlier analysis,
          \begin{displaymath}
            \begin{linsys}{3}
              i_0&- &i_1    &-  &i_2   &=  &0 \\
             -i_0&+ &i_1    &+  &i_2   &=  &0  \\
                 &  &5i_1   &   &      &=  &20  \\
                 &  &       &   &8i_2  &=  &20  \\
                 &  &-5i_1  &+  &8i_2  &=  &0  
            \end{linsys}
          \end{displaymath}
          The current flowing in each branch is then
          is $i_2=20/8=2.5$, $i_1=20/5=4$, and $i_0=13/2=6.5$, all in amperes.
          Thus the parallel portion is acting like a single resistor
          of size $20/(13/2)\approx 3.08$~ohms.
        \partsitem 
          A similar analysis gives that
          is $i_2=i_1=20/8=4$ and $i_0=40/8=5$~ amperes.
          The equivalent resistance is $20/5=4$~ohms.
        \partsitem 
          Another analysis like the prior ones gives 
          is $i_2=20/r_2$, $i_1=20/r_1$, 
          and $i_0=20(r_1+r_2)/(r_1r_2)$, all in amperes.
          So the parallel portion is acting like a single resistor of
          size $20/i_1=r_1r_2/(r_1+r_2)$~ohms.
          (This equation is often stated as:~the equivalent 
          resistance~$r$ satisfies $1/r=(1/r_1)+(1/r_2)$.)
      \end{exparts}
    \end{answer}
% Commented out as bulbs are not linear in resistance as they heat.
%   \item 
%     For the car dashboard example that opens this Topic, solve
%     for these amperages
%     (assume that all resistances are $2$~ohms).
%     \begin{exparts}
%      \partsitem If the driver is stepping on the brakes, so the
%        brake lights are on, and no other circuit is closed.
%      \partsitem If the hi-beam headlights and the brake lights are on.
% %      \partsitem If the hi-beam headlights, brake lights, 
% %        and dome light are all on.
% %        (Perhaps the driver, realizing that the dome light is on and so
% %        that the door must be open, has stepped on the brake!)
%     \end{exparts}
%     \begin{answer}
%       \begin{exparts}
%         \partsitem The circuit looks like this.
%           \begin{center}
%             \includegraphics{ch1.52}
%           \end{center}
%         \partsitem The circuit looks like this.
%           \begin{center}
%             \includegraphics{ch1.53}
%           \end{center}
%       \end{exparts}
%     \end{answer}
\item \label{exer:WheatstoneBr} 
   A \definend{Wheatstone bridge}\index{Wheatstone bridge}
   is used to measure resistance. 
   \begin{center}
     \includegraphics{ch1.47}
   \end{center}
   Show that in this circuit if the current
   flowing through $r_g$ is zero then $r_4=r_2r_3/r_1$.
   (To operate the device, put the unknown
   resistance at $r_4$.
   At $r_g$ is a meter that shows the current.
   We vary the three resistances $r_1$, $r_2$, and $r_3$\Dash typically
   they each have a calibrated knob\Dash until the 
   current in the middle reads $0$.
   Then the equation gives the value of $r_4$.)
   \begin{answer}
     Kirchoff's Current Law, applied to the node where 
     $r_1$, $r_2$, and~$r_g$ come together, and also 
     applied to the node where $r_3$, $r_4$, and~$r_g$ come
     together gives these.
     \begin{equation*}
       \begin{linsys}{3}
         i_1 &- &i_2 &- &i_g &= &0  \\
         i_3 &- &i_4 &+ &i_g &= &0  
       \end{linsys}
     \end{equation*}
     Kirchoff's Voltage law, applied to the loop in the top right, 
     and to the loop in the bottom right, gives these. 
     \begin{equation*}
       \begin{linsys}{3}
         i_3r_3 &- &i_gr_g &- &i_1r_1 &= &0  \\
         i_4r_4 &- &i_2r_2 &+ &i_gr_g &= &0  
       \end{linsys}
     \end{equation*}
     Assuming that $i_g$ is zero gives that $i_1=i_2$, that $i_3=i_4$,
     that $i_1r_1=i_3r_3$, and that $i_2r_2=i_4r_4$.
     Then rearranging the last equality
     \begin{equation*}
       r_4=\frac{i_2r_2}{i_4}\cdot\frac{i_3r_3}{i_1r_1}
     \end{equation*}
     and cancelling the $i$'s gives the desired conclusion.
   \end{answer}
   %  \item Prove Th\`evenin's Theorem.
% \item[]\textit{There are networks other than electrical ones, and 
%               we can ask how well Kirchhoff's laws apply to them.
%               The remaining questions consider an extension to 
%               networks of streets.}
\item 
    Consider this traffic circle.
    \begin{center}
      \includegraphics{ch1.43}
     \end{center}
     This is the traffic volume, in units of cars per five minutes.    
     \begin{center}
       \begin{tabular}{r|ccc}
        \multicolumn{1}{c}{\ } % get rid of vert bar
                            &\textit{North}  
                            &\textit{Pier}  
                            &\textit{Main}  \\
         \cline{2-4}
         \begin{tabular}{@{}r@{}} \textit{into} \\ \textit{out of} \end{tabular}
         &\begin{tabular}{@{}r@{}} $100$ \\ $75$ \end{tabular}
         &\begin{tabular}{@{}r@{}} $150$ \\ $150$ \end{tabular}
         &\begin{tabular}{@{}r@{}} $25$ \\  $50$ \end{tabular}
         % rows:
         % \textit{into}      &$100$           &$150$          &$25$      \\
         % \textit{out of}    &$75$            &$150$          &$50$       
       \end{tabular}
    \end{center}
    We can set up equations to model how the traffic flows.   
    \begin{exparts}
      \partsitem 
         Adapt Kirchhoff's Current Law to this circumstance.
         Is it a reasonable modeling assumption?
      \partsitem 
         Label the three between-road arcs in the circle with a variable.
         Using the (adapted) Current Law,
         for each of the three in-out intersections state an equation
         describing the traffic flow at that node.
      \partsitem 
         Solve that system.
      \partsitem
         Interpret your solution.
      \partsitem
         Restate the Voltage Law for this circumstance.
         How reasonable is it?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem
           An adaptation is:~in any intersection the flow in equals the 
           flow out.
           It does seem reasonable in this case, unless cars are stuck at
           an intersection for a long time.
        \partsitem
           We can label the flow in this way.
           \begin{center}
             \includegraphics{ch1.44}
           \end{center}
           Because $50$ cars leave via Main while $25$~cars enter, 
           $i_1-25=i_2$.
           Similarly Pier's in/out balance means that $i_2=i_3$ and
           North gives $i_3+25=i_1$. 
           We have this system.
           \begin{equation*}
             \begin{linsys}{3}
               i_1  &-  &i_2  &   &     &=  &25  \\
                        &i_2  &-  &i_3  &=  &0   \\
              -i_1  &   &     &+  &i_3  &=  &-25
             \end{linsys}
           \end{equation*}
        \partsitem 
           The row operations $\rho_1+\rho_2$ and $rho_2+\rho_3$ lead
           to the conclusion that there are infinitely many solutions.
           With $i_3$ as the parameter,
           \begin{equation*}
             \set{\colvec[c]{25+i_3  \\ i_3  \\i_3} \suchthat i_3\in\Re}
           \end{equation*}
           of course, since the problem is stated in number of cars, we
           might restrict $i_3$ to be a natural number.
        \partsitem
           If we picture an initially-empty circle with the given input/output
           behavior, we can superimpose a $z_3$-many cars circling endlessly
           to get a new solution.
        \partsitem
           A suitable restatement might be:~the number of cars entering the 
           circle must equal the number of cars leaving.
           The reasonableness of this one is not as clear.
           Over the five minute time period we could find that
           a half dozen more cars entered than left, 
           although the problem statement's into/out table 
           does satisfy this property.
           In any event, it is of no help in getting a unique solution
           since for that we would need to know the number of cars circling
           endlessly.
      \end{exparts}
    \end{answer}
  \item 
     This is a network of streets. 
          \begin{center}
            \includegraphics{ch1.44}
             \end{center}
         We can observe the hourly flow of cars into this network's
         entrances, and out of its exits.
          \begin{center}
            \begin{tabular}{r|ccccc}
               \multicolumn{1}{c}{}  
                 &\textit{east Winooski}  
                 &\textit{west Winooski}  
                 &\textit{Willow}  
                 &\textit{Jay} 
                 &\textit{Shelburne} \\ 
                 \cline{2-6}
              \begin{tabular}{@{}r@{}} \textit{into} \\ \textit{out of} \end{tabular} 
              &\begin{tabular}{@{}r@{}} $80$ \\ $30$ \end{tabular} 
              &\begin{tabular}{@{}r@{}} $50$ \\ $5$ \end{tabular} 
              &\begin{tabular}{@{}r@{}} $65$ \\ $70$ \end{tabular} 
              &\begin{tabular}{@{}r@{}} -- \\ $55$ \end{tabular} 
              &\begin{tabular}{@{}r@{}} $40$ \\ $75$ \end{tabular} 
              % rows:
              % \textit{into}      &80    &50    &65     &--    &40      \\
              % \textit{out of}    &30    &5     &70     &55    &75      
            \end{tabular}
          \end{center}
          (Note that to reach Jay a
          car must enter the network via some other road first, which is why
          there is no `into Jay' entry in the table.
          Note also that over a long period of time, 
          the total in must approximately equal the total 
          out, which is why both rows add to $235$~cars.)
          Once inside the network, the traffic may flow in different
          ways, perhaps filling Willow and leaving Jay 
          mostly empty, or perhaps flowing in some other way.
          Kirchhoff's Laws give the limits on that freedom.
    \begin{exparts}
       \partsitem Determine the restrictions on the flow inside this network 
          of streets by setting
          up a variable for each block, establishing the equations, 
          and solving them.
          Notice that some streets are one-way only.
          (\textit{Hint:}~this will not yield a unique solution, since traffic
          can flow through this network in various ways;
          you should get at least one free variable.)
       \partsitem Suppose that someone proposes construction for
         Winooski Avenue East between Willow and Jay, 
         and traffic on that block will be reduced.
         What is the least amount of traffic flow that can we can
         allow on that block without disrupting the  
         hourly flow into and out of the network?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Here is a variable for each unknown block; each known
          block has the flow shown.
          \begin{center}
            \includegraphics{ch1.51}          
          \end{center}
          We apply Kirchhoff's principle that the flow into the intersection
          of Willow and Shelburne must equal the flow out to get
          $i_1+25=i_2+125$.
          Doing the intersections from right to left and top to bottom
          gives these equations.
          \begin{equation*}
            \begin{linsys}{7}
              i_1 &- &i_2 &  &    &  &    &  &    &  &    &  &    &= &10  \\
             -i_1 &  &    &+ &i_3 &  &    &  &    &  &    &  &    &= &15   \\
                  &  &i_2 &  &    &+ &i_4 &  &    &  &    &  &    &= &5   \\
                  &  &    &  &-i_3&- &i_4 &  &    &+ &i_6 &  &    &= &-50 \\
                  &  &    &  &    &  &    &  &i_5 &  &    &- &i_7 &= &-10 \\
                  &  &    &  &    &  &    &  &    &  &-i_6&+ &i_7 &= &30 
            \end{linsys}
          \end{equation*}
          The row operation $\rho_1+\rho_2$ followed by $\rho_2+\rho_3$
          then $\rho_3+\rho_4$ and $\rho_4+\rho_5$ and finally $\rho_5+\rho_6$
          result in this system.
          \begin{equation*}
            \begin{linsys}{7}
              i_1 &- &i_2 &  &    &  &    &  &    &  &    &  &    &= &10  \\
                  &  &-i_2&+ &i_3 &  &    &  &    &  &    &  &    &= &25   \\
                  &  &    &  &i_3 &+ &i_4 &- &i_5 &  &    &  &    &= &30  \\
                  &  &    &  &    &  &    &  &-i_5&+ &i_6 &  &    &= &-20  \\
                  &  &    &  &    &  &    &  &    &  &-i_6&+ &i_7 &= &-30 \\
                  &  &    &  &    &  &    &  &    &  &    &  &0   &= &0   
            \end{linsys}
          \end{equation*}
          Since the free variables are $i_4$ and $i_7$ we take them as 
          parameters.
          \begin{equation*}
          \begin{split}
            i_6  &=  i_7-30  \\
            i_5  &=  i_6+20=(i_7-30)+20=i_7-10 \\
            i_3  &=  -i_4+i_5+30=-i_4+(i_7-10)+30=-i_4+i_7+20 \\
            i_2  &=  i_3-25=(-i_4+i_7+20)-25=-i_4+i_7-5 \\
            i_1  &=  i_2+10=(-i_4+i_7-5)+10=-i_4+i_7+5
          \end{split}
          \end{equation*}
          Obviously $i_4$ and $i_7$ have to be positive, and in fact
          the first equation shows that $i_7$ must be at least $30$.
          If we start with $i_7$, then the $i_2$~equation shows that
          $0\leq i_4\leq i_7-5$.
        \partsitem We cannot take $i_7$ to be zero or else $i_6$ will
          be negative (this would mean cars going the wrong way on the
          one-way street Jay).
          We can, however, take $i_7$ to be as small as $30$, and then 
          there are many suitable $i_4$'s.
          For instance, the solution
          \begin{equation*}
            (i_1,i_2,i_3,i_4,i_5,i_6,i_7)
            =
            (35,25,50,0,20,0,30)
          \end{equation*}
          results from choosing $i_4=0$.
      \end{exparts}
    \end{answer}
\end{exercises}
\index{networks|)}
\endinput


